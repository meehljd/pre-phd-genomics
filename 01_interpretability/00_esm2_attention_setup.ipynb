{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM Attention Extraction & Visualization\n",
    "## Track A Phase 1: Understanding Foundation Model Behavior\n",
    "\n",
    "**Goal:** Extract and visualize attention patterns from esm2 on genomic sequences\n",
    "\n",
    "**Key Questions:**\n",
    "- What biological signals does esm2 attention capture?\n",
    "- Which positions receive highest attention?\n",
    "- Do attention patterns differ between pathogenic and benign variants?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch & GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Metal (Mac GPU) available: {torch.backends.mps.is_available()}\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create outputs directory\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load ESM2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmModel, EsmTokenizer\n",
    "\n",
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Load tokenizer and model with attention outputs enabled\n",
    "try:\n",
    "    tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "    model = EsmModel.from_pretrained(\n",
    "        model_name,\n",
    "        output_attentions=True,  # CRITICAL: enables attention extraction\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error loading from HF: {e}\")\n",
    "    print(f\"Make sure you have internet connection and the model ID is correct\")\n",
    "    raise\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"✓ Model loaded on {device}\")\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  - Model size: {model_name.split('/')[-1]}\")\n",
    "print(f\"  - Num layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  - Num attention heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  - Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Test on Toy Sequence (100bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create toy amino acid sequence\n",
    "toy_seq = \"MVHLTPEEKS AVTALWGKVN VDEVGGEALG RLLNVLVCVL AHHFGKEFTP PVQAAYQKVV AGVANALAHK\"\n",
    "# Removed spaces for processing:\n",
    "toy_seq = toy_seq.replace(\" \", \"\")\n",
    "\n",
    "print(f\"Test sequence:\")\n",
    "print(f\"  Length: {len(toy_seq)} bp\")\n",
    "print(f\"  Sequence: {toy_seq[:50]}...\")\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer(toy_seq, return_tensors=\"pt\")\n",
    "print(f\"\\nTokenization:\")\n",
    "print(f\"  Input IDs shape: {tokens['input_ids'].shape}\")\n",
    "print(f\"  Token IDs: {tokens['input_ids'].squeeze()[:20]}...\")  # Show first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Forward Pass & Extract Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move tokens to device\n",
    "tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "# Forward pass WITHOUT gradient computation\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "\n",
    "# Extract attention tensors\n",
    "attentions = outputs[-1]  # Tuple of attention tensors, one per layer\n",
    "\n",
    "print(f\"Attention extraction successful!\")\n",
    "print(f\"\\nAttention structure:\")\n",
    "print(f\"  Number of layers: {len(attentions)}\")\n",
    "print(f\"  Each layer shape: (batch, heads, seq_len, seq_len)\")\n",
    "print(f\"\\nFirst 3 layers:\")\n",
    "for i, attn in enumerate(attentions[:3]):\n",
    "    print(f\"  Layer {i}: {attn.shape}\")\n",
    "    # attn.shape = (batch_size=1, num_heads, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Aggregate Attention Across Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_heads(attention_tensor):\n",
    "    \"\"\"\n",
    "    Average attention across heads for cleaner visualization.\n",
    "\n",
    "    Args:\n",
    "        attention_tensor: (batch, heads, seq_len, seq_len)\n",
    "    Returns:\n",
    "        (seq_len, seq_len) aggregated attention matrix\n",
    "    \"\"\"\n",
    "    # Remove batch dimension, average over heads\n",
    "    aggregated = attention_tensor.squeeze(0).mean(dim=0)  # (seq_len, seq_len)\n",
    "    return aggregated.cpu().numpy()\n",
    "\n",
    "\n",
    "# Test on last layer\n",
    "last_layer_attn = attentions[-1]  # (batch, heads, seq_len, seq_len)\n",
    "aggregated = aggregate_heads(last_layer_attn)\n",
    "\n",
    "print(f\"Last layer attention aggregated:\")\n",
    "print(f\"  Shape: {aggregated.shape}\")\n",
    "print(f\"  Mean attention weight: {aggregated.mean():.4f}\")\n",
    "print(f\"  Max attention weight: {aggregated.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Visualize Attention Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot last layer attention\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(aggregated, cmap=\"viridis\", aspect=\"auto\")\n",
    "ax.set_xlabel(\"Attending to position\", fontsize=12)\n",
    "ax.set_ylabel(\"Attending from position\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"esm2 Last Layer Attention (Heads Averaged)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Attention weight\", fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"esm2_toy_attention.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved to {output_dir / 'esm2_toy_attention.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Analyze Attention Across All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all layers\n",
    "all_layers_attn = []\n",
    "for i, attn in enumerate(attentions):\n",
    "    agg = aggregate_heads(attn)\n",
    "    all_layers_attn.append(agg)\n",
    "    print(\n",
    "        f\"Layer {i:2d}: mean={agg.mean():.4f}, max={agg.max():.4f}, diag_mean={agg.diagonal().mean():.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n✓ All {len(all_layers_attn)} layers aggregated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Multi-Layer Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 4 layers\n",
    "n_layers_to_plot = min(4, len(all_layers_attn))\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_layers_to_plot):\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(all_layers_attn[i], cmap=\"viridis\", aspect=\"auto\")\n",
    "    ax.set_title(f\"Layer {i}\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"To\")\n",
    "    ax.set_ylabel(\"From\")\n",
    "    plt.colorbar(im, ax=ax, label=\"Attention\")\n",
    "\n",
    "plt.suptitle(\"ESM2 Attention Across Layers\", fontsize=14, fontweight=\"bold\", y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"esm2_multilayer_attention.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved to {output_dir / 'esm2_multilayer_attention.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Extract Attention on Real Variant (BRCA1 p.R1699W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRCA1 p.R1699W: Known pathogenic variant (Arginine → Tryptophan at position 1699)\n",
    "# Real protein sequence context around the mutation site\n",
    "# Wildtype has R (arginine), variant has W (tryptophan)\n",
    "\n",
    "# Wildtype sequence (normal)\n",
    "brca1_wt = (\n",
    "    \"MDLSALRVEEVQNVINAMQKILECPICLELIKEPVSTKCDHIFCKFCMLKLLNQKKGPSQCPLCKNDITKRSLQESTRFSQLVEELLKIICAFQLD\"\n",
    "    \"TGLEYANSYNFAKKENNSPEHLKDEVSIIQSMGYRNRAKRLLQSEPENPSLQETSLSVQLSNLGTVRTLRTKQRIQPQKTSVYIELGSDSSEDTVN\"\n",
    "    \"KATYCSVGDQELLQITPQGTRDEISLDSAKKAACEFSETDVTNTEHHQPSNNDLNTTEKRAAERHPEKYQGSSVSNLHVEPCGTNTHASSLQHENS\"\n",
    "    \"SLLLTKDRMNVEKAEFCNKSKQPGLARSQHNRWAGSKETCNDRRTPSTEKKVDLNADPLCERKEWNKQKLPCSENPRDTEDVPWITLNSSIQKVNE\"\n",
    "    \"WFSRSDELLGSDDSHDGESESNAKVADVLDVLNEVDEYSGSSEKIDLLASDPHEALICKSERVHSKSVESNIEDKIFGKTYRKKASLPNLSHVTEN\"\n",
    "    \"LIIGAFVTEPQIIQERPLTNKLKRKRRPTSGLHPEDFIKKADLAVQKTPEMINQGTNQTEQNGQVMNITNSGHENKTKGDSIQNEKNPNPIESLEK\"\n",
    "    \"ESAFKTKAEPISSSISNMELELNIHNSKAPKKNRLRRKSSTRHIHALELVVSRNLSPPNCTELQIDSCSSSEEIKKKKYNQMPVRHSRNLQLMEGK\"\n",
    "    \"EPATGAKKSNKPNEQTSKRHDSDTFPELKLTNAPGSFTKCSNTSELKEFVNPSLPREEKEEKLETVKVSNNAEDPKDLMLSGERVLQTERSVESSS\"\n",
    "    \"ISLVPGTDYGTQESISLLEVSTLGKAKTEPNKCVSQCAAFENPKGLIHGCSKDNRNDTEGFKYPLGHEVNHSRETSIEMEESELDAQYLQNTFKVS\"\n",
    "    \"KRQSFAPFSNPGNAEEECATFSAHSGSLKKQSPKVTFECEQKEENQGKNESNIKPVQTVNITAGFPVVGQKDKPVDNAKCSIKGGSRFCLSSQFRG\"\n",
    "    \"NETGLITPNKHGLLQNPYRIPPLFPIKSFVKTKCKKNLLEENFEEHSMSPEREMGNENIPSTVSTISRNNIRENVFKEASSSNINEVGSSTNEVGS\"\n",
    "    \"SINEIGSSDENIQAELGRNRGPKLNAMLRLGVLQPEVYKQSLPGSNCKHPEIKKQEYEEVVQTVNTDFSPYLISDNLEQPMGSSHASQVCSETPDD\"\n",
    "    \"LLDDGEIKEDTSFAENDIKESSAVFSKSVQKGELSRSPSPFTHTHLAQGYRRGAKKLESSEENLSSEDEELPCFQHLLFGKVNNIPSQSTRHSTVA\"\n",
    "    \"TECLSKNTEENLLSLKNSLNDCSNQVILAKASQEHHLSEETKCSASLFSSQCSELEDLTANTNTQDPFLIGSSKQMRHQSESQGVGLSDKELVSDD\"\n",
    "    \"EERGTGLEENNQEEQSMDSNLGEAASGCESETSVSEDCSGLSSQSDILTTQQRDTMQHNLIKLQQEMAELEAVLEQHGSQPSNSYPSIISDSSALE\"\n",
    "    \"DLRNPEQSTSEKAVLTSQKSSEYPISQNPEGLSADKFEVSADSSTSKNKEPGVERSSPSKCPSLDDRWYMHSCSGSLQNRNYPSQEELIKVVDVEE\"\n",
    "    \"QQLEESGPHDLTETSYLPRQDLEGTPYLESGISLFSDDPESDPSEDRAPESARVGNIPSSTSALKVPQLKVAESAQSPAAAHTTDTAGYNAMEESV\"\n",
    "    \"SREKPELTASTERVNKRMSMVVSGLTPEEFMLVYKFARKHHITLTNLITEETTHVVMKTDAEFVCERTLKYFLGIAGGKWVVSYFWVTQSIKERKM\"\n",
    "    \"LNEHDFEVRGDVVNGRNHQGPKRARESQDRKIFRGLEICCYGPFTNMPTDQLEWMVQLCGASVVKELSSFTLGTGVHPIVVVQPDAWTEDNGFHAI\"\n",
    "    \"GQMCEAPVVTREWVLDSVALYQCQELDTYLIPQIPHSHY\"\n",
    ")\n",
    "# Variant sequence (R1699W mutation - replace R with W at mutation position)\n",
    "mutation_pos = 1699  # Approximate\n",
    "brca1_variant = brca1_wt[:1698] + \"W\" + brca1_wt[1700:]\n",
    "\n",
    "# Annotate mutation position (~1699)\n",
    "brca1_seq = brca1_variant  # Analyze the variant sequence\n",
    "\n",
    "print(f\"WT length: {len(brca1_wt)} AA\")\n",
    "print(f\"Variant length: {len(brca1_variant)} AA\")\n",
    "print(f\"Difference: {brca1_wt != brca1_variant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokens_brca = tokenizer(brca1_seq, return_tensors=\"pt\")\n",
    "tokens_brca = {k: v.to(device) for k, v in tokens_brca.items()}\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs_wt = model(**tokens_brca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions_wt = outputs_wt[-1]\n",
    "# outputs_wt is a TUPLE containing:\n",
    "# [0] = hidden states (embeddings)\n",
    "# [1] = logits (predictions)\n",
    "# [2:] = attentions (if output_attentions=True)\n",
    "\n",
    "# Extract last layer\n",
    "layer_wt = aggregate_heads(attentions_wt[32])\n",
    "# attentions_wt is a TUPLE with one tensor per layer\n",
    "# Shape of each: (batch=1, heads=12, seq_len, seq_len)\n",
    "\n",
    "layer_wt = (layer_wt + layer_wt.T) / 2\n",
    "\n",
    "print(f\"\\n✓ BRCA1 variant attention extracted\")\n",
    "print(f\"  Attention shape: {layer_wt.shape}\")\n",
    "print(f\"  Mean attention: {layer_wt.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Visualize BRCA1 Variant Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_wt_trim = layer_wt[1600:1800, 1600:1800]\n",
    "\n",
    "mut_pos_trim = mutation_pos - 1 - 1600  # Adjust for zero-based indexing\n",
    "\n",
    "# Plot BRCA1 variant attention\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(layer_wt_trim, cmap=\"viridis\", aspect=\"auto\")\n",
    "ax.set_xlabel(\"Attending to position\", fontsize=12)\n",
    "ax.set_ylabel(\"Attending from position\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"ESM2 Attention: BRCA1 R1699W (Pathogenic Variant)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Attention weight\", fontsize=11)\n",
    "\n",
    "ax.axhline(\n",
    "    mut_pos_trim,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    alpha=0.7,\n",
    "    label=\"Variant position\",\n",
    ")\n",
    "ax.axvline(mut_pos_trim, color=\"red\", linestyle=\"--\", linewidth=2, alpha=0.7)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"esm2_brca1_attention.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved to {output_dir / 'esm2_brca1_attention.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe\n",
    "summary_data = []\n",
    "\n",
    "for i, attn in enumerate(all_layers_attn):\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Layer\": i,\n",
    "            \"Mean Attention\": attn.mean(),\n",
    "            \"Max Attention\": attn.max(),\n",
    "            \"Std Attention\": attn.std(),\n",
    "            \"Diagonal Mean\": attn.diagonal().mean(),  # Self-attention\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n=== Attention Summary Across Layers ===\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(output_dir / \"attention_summary.csv\", index=False)\n",
    "print(f\"\\n✓ Summary saved to {output_dir / 'attention_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Key Observations & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"\n",
    "=== WEEK 1 SUMMARY ===\n",
    "\n",
    "✓ Successfully extracted attention from esm2 foundation model\n",
    "✓ Visualized attention patterns across layers\n",
    "✓ Tested on both toy and real variant sequences\n",
    "\n",
    "KEY OBSERVATIONS:\n",
    "- Diagonal values (self-attention) show how much each position attends to itself\n",
    "- Attention patterns change across layers (early vs late layers)\n",
    "- Variant-specific context affects attention weights\n",
    "\n",
    "NEXT STEPS (Week 2-3):\n",
    "1. Analyze 10 variants (5 pathogenic, 5 benign)\n",
    "2. Compare attention patterns between pathogenic vs benign\n",
    "3. Validate: Do positions of known functional domains get high attention?\n",
    "4. Begin SHAP analysis (Phase 2)\n",
    "\n",
    "FILES CREATED:\n",
    "- outputs/esm2_toy_attention.png\n",
    "- outputs/esm2_multilayer_attention.png\n",
    "- outputs/esm2_brca1_attention.png\n",
    "- outputs/attention_summary.csv\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
