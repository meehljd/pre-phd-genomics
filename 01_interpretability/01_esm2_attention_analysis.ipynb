{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ef744",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import EsmModel, EsmTokenizer\n",
    "\n",
    "from variant_dataset import PATHOGENIC_VARIANTS, get_variant_sequence\n",
    "from utils import (\n",
    "    extract_attention, \n",
    "    plot_attention_heatmap, \n",
    "    get_attention_stats, \n",
    "    plot_attention_difference, \n",
    "    analyze_variant_attention_changes, \n",
    "    plot_variant_attention_profile,\n",
    "    compare_attention_layers_difference,\n",
    "    compare_attention_layers_overlay,\n",
    "    analyze_layer_wise_changes,\n",
    "    extract_attention_per_head,\n",
    "    find_most_different_heads,\n",
    "    compare_specific_heads,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch & GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Metal (Mac GPU) available: {torch.backends.mps.is_available()}\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create outputs directory\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22dbd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Load tokenizer and model with attention outputs enabled\n",
    "try:\n",
    "    tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "    model = EsmModel.from_pretrained(\n",
    "        model_name,\n",
    "        output_attentions=True,  # CRITICAL: enables attention extraction\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error loading from HF: {e}\")\n",
    "    print(f\"Make sure you have internet connection and the model ID is correct\")\n",
    "    raise\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"✓ Model loaded on {device}\")\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  - Model size: {model_name.split('/')[-1]}\")\n",
    "print(f\"  - Num layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  - Num attention heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  - Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich variants with sequences\n",
    "variants = []\n",
    "for var in PATHOGENIC_VARIANTS:\n",
    "    enriched_var = var.copy()  # Don't modify original\n",
    "\n",
    "    # Fetch sequences\n",
    "    enriched_var[\"seq_wt\"] = get_variant_sequence(var, version=\"wt\")\n",
    "    enriched_var[\"seq_mut\"] = get_variant_sequence(var, version=\"mut\")\n",
    "\n",
    "    variants.append(enriched_var)\n",
    "    print(f\"✓ Loaded {var['gene']} {var['wt']}{var['pos']}{var['mut']}\")\n",
    "\n",
    "# Now each variant has: gene, uniprot, pos, wt, mut, seq_wt, seq_mut\n",
    "# variants = pd.DataFrame(variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28dc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_variant(var):\n",
    "    print(f\"Analyzing: {var['gene']} {var['wt']}{var['pos']}{var['mut']}\")\n",
    "\n",
    "    # Extract attention from last layer\n",
    "    wt_attn = extract_attention(\n",
    "        model, tokenizer, var[\"seq_wt\"], layer_idx=-1, device=device\n",
    "    )\n",
    "\n",
    "    mut_attn = extract_attention(\n",
    "        model, tokenizer, var[\"seq_mut\"], layer_idx=-1, device=device\n",
    "    )\n",
    "\n",
    "    print(f\"WT attention shape: {wt_attn.shape}\")\n",
    "    print(f\"Mut attention shape: {mut_attn.shape}\")\n",
    "\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # # WT attention\n",
    "    # plot_attention_heatmap(\n",
    "    #     wt_attn,\n",
    "    #     title=f\"{var['gene']} WT - {var['wt']}{var['pos']}\",\n",
    "    #     variant_pos=var[\"pos\"],\n",
    "    #     ax=axes[0],\n",
    "    #     window=50,\n",
    "    # )\n",
    "\n",
    "    # # Mutant attention\n",
    "    # plot_attention_heatmap(\n",
    "    #     mut_attn,\n",
    "    #     title=f\"{var['gene']} Mutant - {var['mut']}{var['pos']}\",\n",
    "    #     variant_pos=var[\"pos\"],\n",
    "    #     ax=axes[1],\n",
    "    #     window=50,\n",
    "    # )\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"outputs/{var['gene']}_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    # plt.show()\n",
    "\n",
    "    # 2. Quantitative analysis\n",
    "    changes = analyze_variant_attention_changes(wt_attn, mut_attn, var[\"pos\"])\n",
    "    print(\"\\nAttention Changes:\")\n",
    "    for key, val in changes.items():\n",
    "        print(f\"{key}: {val}\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plot_attention_difference(\n",
    "        wt_attn,\n",
    "        mut_attn,\n",
    "        f\"{var['gene']} {var['wt']}{var['pos']}{var['mut']} - Attention Change\",\n",
    "        variant_pos=var[\"pos\"],\n",
    "        window=50,\n",
    "        ax=ax,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # wt_stats = get_attention_stats(wt_attn)\n",
    "    # mut_stats = get_attention_stats(mut_attn)\n",
    "\n",
    "    # print(f\"\\n{var['gene']} {var['wt']}{var['pos']}{var['mut']} Statistics:\")\n",
    "    # print(f\"{'Metric':<20} {'WT':<15} {'Mutant':<15} {'Δ':<15}\")\n",
    "    # print(\"-\" * 65)\n",
    "    # for key in wt_stats.keys():\n",
    "    #     delta = mut_stats[key] - wt_stats[key]\n",
    "    #     print(\n",
    "    #         f\"{key:<20} {wt_stats[key]:<15.4f} {mut_stats[key]:<15.4f} {delta:<15.4f}\"\n",
    "    #     )\n",
    "\n",
    "    # 3. Profile plots (attention vectors)\n",
    "    plot_variant_attention_profile(wt_attn, mut_attn, var[\"pos\"])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d4fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_variant(var):\n",
    "    \"\"\"Analyze a single variant, handling indels.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {var['gene']} {var['wt']}{var['pos']}{var['mut']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Determine variant type\n",
    "    if var[\"mut\"] == \"del\":\n",
    "        variant_type = \"deletion\"\n",
    "    elif var[\"mut\"].startswith(\"ins\"):\n",
    "        variant_type = \"insertion\"\n",
    "    else:\n",
    "        variant_type = \"substitution\"\n",
    "\n",
    "    # Extract attention\n",
    "    wt_attn = extract_attention(\n",
    "        model, tokenizer, var[\"seq_wt\"], layer_idx=-1, device=device\n",
    "    )\n",
    "    mut_attn = extract_attention(\n",
    "        model, tokenizer, var[\"seq_mut\"], layer_idx=-1, device=device\n",
    "    )\n",
    "\n",
    "    print(f\"Variant type: {variant_type}\")\n",
    "    print(f\"WT shape: {wt_attn.shape}, Mut shape: {mut_attn.shape}\")\n",
    "\n",
    "    # Quantitative analysis\n",
    "    changes = analyze_variant_attention_changes(\n",
    "        wt_attn, mut_attn, var[\"pos\"], variant_type=variant_type\n",
    "    )\n",
    "\n",
    "    print(\"\\nAttention Changes:\")\n",
    "    for key, val in changes.items():\n",
    "        print(f\"  {key}: {val}\")\n",
    "\n",
    "    # For substitutions, also show difference\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_attention_difference(\n",
    "        wt_attn,\n",
    "        mut_attn,\n",
    "        f\"{var['gene']} {var['wt']}{var['pos']}{var['mut']}\",\n",
    "        variant_pos=var[\"pos\"],\n",
    "        variant_type=variant_type,\n",
    "        ax=ax,\n",
    "        window=50,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82364b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_variant(variants[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_variant(variants[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_variant(variants[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_variant(variants[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2317e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_variant(variants[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention from ALL layers (layer_idx=None)\n",
    "var = variants[0]\n",
    "\n",
    "wt_all_layers = extract_attention(\n",
    "    model, tokenizer, var[\"seq_wt\"], layer_idx=None, device=device  # Get all layers!\n",
    ")\n",
    "\n",
    "mut_all_layers = extract_attention(\n",
    "    model, tokenizer, var[\"seq_mut\"], layer_idx=None, device=device\n",
    ")\n",
    "\n",
    "print(f\"WT attention shape: {wt_all_layers.shape}\")  # [33 layers, seq_len, seq_len]\n",
    "print(f\"Mut attention shape: {mut_all_layers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Difference plots (shows delta across layers)\n",
    "compare_attention_layers_difference(\n",
    "    wt_all_layers,\n",
    "    mut_all_layers,\n",
    "    title=f\"{var['gene']} {var['wt']}{var['pos']}{var['mut']} - Layer Changes\",\n",
    "    save_path=f\"outputs/{var['gene']}_layer_differences.png\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 2. Overlay plots (direct comparison)\n",
    "compare_attention_layers_overlay(\n",
    "    wt_all_layers,\n",
    "    mut_all_layers,\n",
    "    title=f\"{var['gene']} {var['wt']}{var['pos']}{var['mut']} - WT vs Mutant\",\n",
    "    save_path=f\"outputs/{var['gene']}_layer_overlay.png\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 3. Quantitative analysis\n",
    "layer_analysis = analyze_layer_wise_changes(wt_all_layers, mut_all_layers)\n",
    "print(f\"\\nMost changed layers: {layer_analysis['most_changed_layers']}\")\n",
    "print(\n",
    "    f\"Mean change across all layers: {layer_analysis['mean_change_across_layers']:.6f}\"\n",
    ")\n",
    "\n",
    "# Look at specific layer changes\n",
    "for change in layer_analysis[\"layer_changes\"][:10]:  # First 10 layers\n",
    "    print(f\"Layer {change['layer']}: mean_abs_change={change['mean_abs_change']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_layer = 10\n",
    "n_plot = 3\n",
    "\n",
    "\n",
    "# Step 1: Start with layer averages (current approach)\n",
    "wt_all_layers = extract_attention(\n",
    "    model, tokenizer, var[\"seq_wt\"], layer_idx=None, device=device\n",
    ")\n",
    "mut_all_layers = extract_attention(\n",
    "    model, tokenizer, var[\"seq_mut\"], layer_idx=None, device=device\n",
    ")\n",
    "\n",
    "compare_attention_layers_difference(\n",
    "    wt_all_layers, mut_all_layers, title=f\"{var['gene']} - Find interesting layers\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 3: Extract per-head for that layer\n",
    "wt_heads = extract_attention_per_head(\n",
    "    model, tokenizer, var[\"seq_wt\"], layer_idx=interesting_layer, device=device\n",
    ")\n",
    "mut_heads = extract_attention_per_head(\n",
    "    model, tokenizer, var[\"seq_mut\"], layer_idx=interesting_layer, device=device\n",
    ")\n",
    "\n",
    "print(f\"Shape per head: {wt_heads.shape}\")  # [20, seq_len, seq_len] for ESM2-650M\n",
    "\n",
    "# Step 4: Find which heads changed most\n",
    "top_heads = find_most_different_heads(wt_heads, mut_heads, top_k=10)\n",
    "print(f\"\\nTop 3 most different heads in layer {interesting_layer}:\")\n",
    "for head_idx, diff in top_heads:\n",
    "    print(f\"  Head {head_idx}: mean abs diff = {diff:.6f}\")\n",
    "\n",
    "plt.plot(\n",
    "    [h for h, _ in top_heads[:n_plot]], [d for _, d in top_heads[:n_plot]], marker=\"o\"\n",
    ")\n",
    "plt.xlabel(\"Head\")\n",
    "plt.ylabel(\"Mean Abs Diff\")\n",
    "plt.title(f\"Top {n_plot} Most Different Heads in Layer {interesting_layer}\")\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Visualize those specific heads\n",
    "compare_specific_heads(\n",
    "    wt_heads,\n",
    "    mut_heads,\n",
    "    head_indices=[h for h, _ in top_heads[:n_plot]],\n",
    "    layer_idx=interesting_layer,\n",
    "    variant_info=var,\n",
    "    window=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639ed0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
