{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Contrastive Learning for Debiasing Genomic Embeddings\n\n## Overview\n\nThis notebook implements supervised contrastive learning to improve pre-computed genomic embeddings by reducing confounding effects from ancestry and technical variables while preserving discriminative signal for the outcome of interest.\n\n## Background\n\nGenomic data often contains confounding factors (ancestry, sequencing batch, read depth, etc.) that can lead to spurious associations in downstream analyses. Traditional approaches like linear regression adjustment or propensity score matching have limitations. Contrastive learning offers an alternative: learn an embedding space where:\n\n1. **Samples with the same phenotype** are pulled together (positive pairs)\n2. **Samples with different phenotypes** but matched confounders are pushed apart (negative pairs)\n3. **Confounder effects** are minimized through the matched design\n\n## Study Design\n\nWe use a **matched case-control design** with:\n- Cases: Samples with `is_positive = 1`\n- Controls: 4 matched controls per case (matched on confounders)\n- Each case-control group is identified by `case_matched` column\n\n### Contrastive Pairs Definition\n\n- **Positive pairs**: Two samples with the same `is_positive` label AND same `case_matched` group\n- **Negative pairs**: Two samples with different `is_positive` labels AND same `case_matched` group\n\nThis ensures we're learning to discriminate between cases and controls while being invariant to the confounders they share.\n\n## Training Strategy\n\n1. Split data by `case_matched` groups (20% validation) to prevent leakage\n2. Train a projection network using supervised contrastive loss\n3. Monitor progress using cluster separation and logistic regression AUC\n4. Compare embeddings before and after training using PCA visualizations\n\n## Expected Outcome\n\nThe trained embeddings should:\n- ✓ Improve separation between cases and controls (higher AUC)\n- ✓ Reduce correlation with confounding variables\n- ✓ Maintain or improve downstream predictive performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, calinski_harabasz_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Data Loading\n\nLoad the parquet file containing pre-computed embeddings and metadata. The data should include:\n- **Embeddings**: Pre-computed feature vectors (e.g., from a variant autoencoder, PRS, or other genomic model)\n- **is_positive**: Binary outcome label (1 = case, 0 = control)\n- **case_matched**: Group ID linking each case to its matched controls\n- **Confounders**: Variables to control for (ancestry, batch, read depth, etc.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet file\n",
    "# UPDATE THIS PATH to your actual parquet file\n",
    "parquet_path = \"path/to/your/embeddings.parquet\"\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['is_positive'].value_counts())\n",
    "print(f\"\\nUnique case_matched values: {df['case_matched'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embedding columns (assume they start with 'emb_' or 'embedding_' or are numeric)\n",
    "# UPDATE THIS based on your column naming convention\n",
    "embedding_cols = [col for col in df.columns if col.startswith('emb_') or col.startswith('embedding_')]\n",
    "if len(embedding_cols) == 0:\n",
    "    # If no prefix, assume all numeric columns except known metadata are embeddings\n",
    "    metadata_cols = ['is_positive', 'case_matched', 'case_id']\n",
    "    embedding_cols = [col for col in df.select_dtypes(include=[np.number]).columns \n",
    "                     if col not in metadata_cols]\n",
    "\n",
    "print(f\"Found {len(embedding_cols)} embedding dimensions\")\n",
    "embeddings = df[embedding_cols].values\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Initial PCA Analysis and Visualization\n\nBefore training, we visualize the original embeddings using PCA to understand:\n1. How much variance is captured by PC1 and PC2\n2. Whether cases and controls are already separated\n3. How confounders correlate with the embedding space\n\n**Goal**: Identify if confounders are driving the main axes of variation, which would indicate bias that needs correction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on original embeddings\n",
    "pca_original = PCA(n_components=2)\n",
    "pca_coords_original = pca_original.fit_transform(embeddings)\n",
    "\n",
    "# Add PCA coordinates to dataframe\n",
    "df['PC1_original'] = pca_coords_original[:, 0]\n",
    "df['PC2_original'] = pca_coords_original[:, 1]\n",
    "\n",
    "print(f\"Explained variance ratio: {pca_original.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca_original.explained_variance_ratio_.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify confounder columns (UPDATE THESE based on your data)\n",
    "# Examples: ancestry, batch, read_depth, sequencing_platform, etc.\n",
    "confounder_cols = [col for col in df.columns if col in [\n",
    "    'ancestry', 'batch', 'read_depth', 'sequencing_platform', \n",
    "    'age', 'sex', 'pc1', 'pc2', 'pc3'  # Add your actual confounder column names\n",
    "]]\n",
    "\n",
    "print(f\"Confounder columns found: {confounder_cols}\")\n",
    "if len(confounder_cols) == 0:\n",
    "    print(\"WARNING: No confounder columns found. Please update the confounder_cols list above.\")\n",
    "    # Create dummy example for demonstration\n",
    "    confounder_cols = ['is_positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-plot visualization of PC1 and PC2 vs confounders\n",
    "n_confounders = len(confounder_cols) + 1  # +1 for the main is_positive plot\n",
    "n_cols = min(3, n_confounders)\n",
    "n_rows = (n_confounders + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "axes = axes.flatten() if n_confounders > 1 else [axes]\n",
    "\n",
    "# Plot PC1 vs PC2 colored by is_positive (main outcome)\n",
    "scatter = axes[0].scatter(df['PC1_original'], df['PC2_original'], \n",
    "                         c=df['is_positive'], cmap='coolwarm', \n",
    "                         alpha=0.6, s=20)\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].set_title('PC1 vs PC2 (Original) - Colored by is_positive')\n",
    "axes[0].legend(*scatter.legend_elements(), title=\"is_positive\")\n",
    "\n",
    "# Plot PC1 vs PC2 colored by each confounder\n",
    "for idx, confounder in enumerate(confounder_cols, start=1):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    \n",
    "    # Check if confounder is categorical or continuous\n",
    "    if df[confounder].dtype == 'object' or df[confounder].nunique() < 10:\n",
    "        # Categorical - use discrete colors\n",
    "        scatter = axes[idx].scatter(df['PC1_original'], df['PC2_original'], \n",
    "                                   c=pd.Categorical(df[confounder]).codes, \n",
    "                                   cmap='tab10', alpha=0.6, s=20)\n",
    "        axes[idx].legend(*scatter.legend_elements(), title=confounder, \n",
    "                        loc='best', fontsize=8)\n",
    "    else:\n",
    "        # Continuous - use continuous colormap\n",
    "        scatter = axes[idx].scatter(df['PC1_original'], df['PC2_original'], \n",
    "                                   c=df[confounder], cmap='viridis', \n",
    "                                   alpha=0.6, s=20)\n",
    "        plt.colorbar(scatter, ax=axes[idx], label=confounder)\n",
    "    \n",
    "    axes[idx].set_xlabel('PC1')\n",
    "    axes[idx].set_ylabel('PC2')\n",
    "    axes[idx].set_title(f'PC1 vs PC2 (Original) - Colored by {confounder}')\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_confounders, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_original_confounders.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Train/Validation Split\n\n**Critical**: We split by `case_matched` groups (not individual samples) to prevent data leakage.\n\nIf we randomly split individuals, a case and its matched controls might end up in different sets, allowing the model to \"cheat\" by learning confounder patterns that appear in both train and validation.\n\nBy keeping each case-control group together, we ensure the model generalizes to unseen confounder combinations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique case_matched groups (each represents a case + its 4 matched controls)\n",
    "unique_groups = df['case_matched'].unique()\n",
    "print(f\"Total unique case groups: {len(unique_groups)}\")\n",
    "\n",
    "# Split groups into train/val (20% validation)\n",
    "train_groups, val_groups = train_test_split(unique_groups, test_size=0.2, random_state=42)\n",
    "print(f\"Train groups: {len(train_groups)}, Val groups: {len(val_groups)}\")\n",
    "\n",
    "# Create train and validation masks\n",
    "train_mask = df['case_matched'].isin(train_groups)\n",
    "val_mask = df['case_matched'].isin(val_groups)\n",
    "\n",
    "df_train = df[train_mask].copy()\n",
    "df_val = df[val_mask].copy()\n",
    "\n",
    "print(f\"\\nTrain set: {len(df_train)} samples ({df_train['is_positive'].sum()} positive)\")\n",
    "print(f\"Val set: {len(df_val)} samples ({df_val['is_positive'].sum()} positive)\")\n",
    "\n",
    "# Extract embeddings for train and val\n",
    "X_train = df_train[embedding_cols].values\n",
    "y_train = df_train['is_positive'].values\n",
    "X_val = df_val[embedding_cols].values\n",
    "y_val = df_val['is_positive'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. PyTorch Contrastive Learning Architecture\n\n### ContrastiveProjector\nA 3-layer MLP that projects the input embeddings to a lower-dimensional space (128D) optimized for contrastive learning:\n- **BatchNorm**: Stabilizes training and prevents internal covariate shift\n- **Dropout**: Prevents overfitting to specific confounder patterns\n- **L2 Normalization**: Output embeddings lie on unit hypersphere, making cosine similarity meaningful\n\n### ContrastiveLoss\nSupervised contrastive loss adapted for matched case-control design:\n- Uses temperature-scaled cosine similarity\n- Only considers pairs within the same `case_matched` group\n- Pulls together samples with same label, pushes apart samples with different labels\n- Automatically handles variable numbers of positive pairs per sample"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Projection network for contrastive learning.\n",
    "    Takes pre-computed embeddings and projects them to a lower-dimensional space\n",
    "    optimized for discriminating between positive and negative cases while\n",
    "    being invariant to confounders.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=128, dropout=0.1):\n",
    "        super(ContrastiveProjector, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.normalize(self.network(x), dim=1)  # L2 normalize for better contrastive learning\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised contrastive loss for matched case-control design.\n",
    "    Positive pairs: same is_positive label AND same case_matched group\n",
    "    Negative pairs: different is_positive label AND same case_matched group (matched controls)\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, features, labels, case_matched):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: normalized embeddings, shape (batch_size, embed_dim)\n",
    "            labels: is_positive labels, shape (batch_size,)\n",
    "            case_matched: case_matched group IDs, shape (batch_size,)\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        \n",
    "        # Create masks for positive and negative pairs\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        case_matched = case_matched.contiguous().view(-1, 1)\n",
    "        \n",
    "        # Positive mask: same label AND same case_matched group (but not same sample)\n",
    "        label_mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        case_mask = torch.eq(case_matched, case_matched.T).float().to(device)\n",
    "        positive_mask = label_mask * case_mask\n",
    "        positive_mask.fill_diagonal_(0)  # Exclude self-comparisons\n",
    "        \n",
    "        # Negative mask: different label AND same case_matched group\n",
    "        negative_mask = (1 - label_mask) * case_mask\n",
    "        \n",
    "        # For numerical stability\n",
    "        logits_max, _ = torch.max(similarity_matrix, dim=1, keepdim=True)\n",
    "        logits = similarity_matrix - logits_max.detach()\n",
    "        \n",
    "        # Compute log probabilities\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(batch_size).to(device))  # Exclude diagonal\n",
    "        \n",
    "        # Only consider negatives from same case_matched group\n",
    "        log_prob = logits - torch.log(torch.sum(exp_logits * (positive_mask + negative_mask + 1e-8), dim=1, keepdim=True))\n",
    "        \n",
    "        # Compute mean of log-likelihood over positive pairs\n",
    "        mean_log_prob_pos = (positive_mask * log_prob).sum(1) / (positive_mask.sum(1) + 1e-8)\n",
    "        \n",
    "        # Loss is negative log-likelihood\n",
    "        loss = -mean_log_prob_pos\n",
    "        loss = loss[positive_mask.sum(1) > 0].mean()  # Only compute loss for samples with positive pairs\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Initialize model\n",
    "input_dim = len(embedding_cols)\n",
    "model = ContrastiveProjector(input_dim=input_dim, hidden_dim=256, output_dim=128).to(device)\n",
    "criterion = ContrastiveLoss(temperature=0.07)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Dataset and DataLoader\n\nPyTorch dataset wrapper for the embeddings with custom collation to pass:\n1. Pre-computed embeddings (input features)\n2. Labels (`is_positive`)\n3. Case-match group IDs (for contrastive loss computation)\n\nThe DataLoader shuffles training data while keeping batches reasonably sized to ensure diverse case-control groups per batch."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    \"\"\"Dataset for pre-computed embeddings with labels and case_matched groups\"\"\"\n",
    "    def __init__(self, embeddings, labels, case_matched):\n",
    "        self.embeddings = torch.FloatTensor(embeddings)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.case_matched = torch.LongTensor(case_matched)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx], self.case_matched[idx]\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EmbeddingDataset(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    df_train['case_matched'].values\n",
    ")\n",
    "\n",
    "val_dataset = EmbeddingDataset(\n",
    "    X_val, \n",
    "    y_val, \n",
    "    df_val['case_matched'].values\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "# Use batch size that captures multiple cases with their controls (5 samples per case * batch)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Monitoring Functions\n\nWe track two complementary metrics during training:\n\n### 1. Calinski-Harabasz Index (Geometric)\nRatio of between-cluster variance to within-cluster variance. Higher values indicate better-defined, more separated clusters.\n- **Formula**: (BSS / WSS) × ((N - k) / (k - 1)) where N = samples, k = classes\n- **Advantage**: Fast, well-established, scale-invariant\n- **Range**: [0, ∞), higher is better\n\n### 2. Logistic Regression AUC (Discriminative)\nTrain a simple linear classifier on the embeddings and evaluate on validation set.\n- **Advantage**: Measures actual downstream predictive utility\n- **Limitation**: May underestimate quality if embeddings require non-linear classifier\n\nBy monitoring both metrics, we get a complete picture of embedding quality."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_cluster_separation(embeddings, labels):\n    \"\"\"\n    Compute Calinski-Harabasz Index (Variance Ratio Criterion).\n    Measures the ratio of between-cluster to within-cluster variance.\n    Higher values indicate better-defined clusters.\n    \n    Returns:\n        float: CH index score (higher is better, range [0, inf))\n    \"\"\"\n    if len(np.unique(labels)) < 2:\n        return 0.0\n    \n    return calinski_harabasz_score(embeddings, labels)\n\n\ndef compute_logistic_auc(X_train, y_train, X_val, y_val):\n    \"\"\"\n    Train a logistic regression classifier and return validation AUC.\n    \"\"\"\n    clf = LogisticRegression(max_iter=1000, random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Predict probabilities\n    y_pred_proba = clf.predict_proba(X_val)[:, 1]\n    auc = roc_auc_score(y_val, y_pred_proba)\n    \n    return auc\n\n\ndef evaluate_embeddings(model, train_loader, val_loader, device):\n    \"\"\"\n    Evaluate the quality of learned embeddings using cluster separation and AUC.\n    \"\"\"\n    model.eval()\n    \n    # Get all train embeddings\n    train_embeddings_list = []\n    train_labels_list = []\n    with torch.no_grad():\n        for embeddings, labels, _ in train_loader:\n            embeddings = embeddings.to(device)\n            projected = model(embeddings)\n            train_embeddings_list.append(projected.cpu().numpy())\n            train_labels_list.append(labels.numpy())\n    \n    train_embeddings = np.vstack(train_embeddings_list)\n    train_labels = np.concatenate(train_labels_list)\n    \n    # Get all val embeddings\n    val_embeddings_list = []\n    val_labels_list = []\n    with torch.no_grad():\n        for embeddings, labels, _ in val_loader:\n            embeddings = embeddings.to(device)\n            projected = model(embeddings)\n            val_embeddings_list.append(projected.cpu().numpy())\n            val_labels_list.append(labels.numpy())\n    \n    val_embeddings = np.vstack(val_embeddings_list)\n    val_labels = np.concatenate(val_labels_list)\n    \n    # Compute metrics\n    train_separation = compute_cluster_separation(train_embeddings, train_labels)\n    val_separation = compute_cluster_separation(val_embeddings, val_labels)\n    auc = compute_logistic_auc(train_embeddings, train_labels, val_embeddings, val_labels)\n    \n    return {\n        'train_separation': train_separation,\n        'val_separation': val_separation,\n        'val_auc': auc,\n        'train_embeddings': train_embeddings,\n        'train_labels': train_labels,\n        'val_embeddings': val_embeddings,\n        'val_labels': val_labels\n    }\n\nprint(\"Monitoring functions defined successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Training Loop\n\nTrain the projection network with:\n- **AdamW optimizer**: Decoupled weight decay for better generalization\n- **Cosine annealing schedule**: Gradually reduces learning rate for fine-tuning\n- **Evaluation every 5 epochs**: Track cluster separation and AUC without slowing training\n- **Best model selection**: Save model with highest validation AUC\n\n**Note**: Some batches may have NaN loss if they don't contain valid positive pairs. These are safely skipped."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_separation': [],\n",
    "    'val_separation': [],\n",
    "    'val_auc': []\n",
    "}\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs...\")\n",
    "print(f\"Initial evaluation before training:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline evaluation (before training)\n",
    "baseline_metrics = evaluate_embeddings(model, train_loader, val_loader, device)\n",
    "print(f\"Baseline - Train Separation: {baseline_metrics['train_separation']:.4f}\")\n",
    "print(f\"Baseline - Val Separation: {baseline_metrics['val_separation']:.4f}\")\n",
    "print(f\"Baseline - Val AUC: {baseline_metrics['val_auc']:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "best_val_auc = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for embeddings, labels, case_matched in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        case_matched = case_matched.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        projected = model(embeddings)\n",
    "        loss = criterion(projected, labels, case_matched)\n",
    "        \n",
    "        # Skip if loss is nan (can happen if batch has no valid pairs)\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "    \n",
    "    train_loss /= max(train_batches, 1)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels, case_matched in val_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "            case_matched = case_matched.to(device)\n",
    "            \n",
    "            projected = model(embeddings)\n",
    "            loss = criterion(projected, labels, case_matched)\n",
    "            \n",
    "            if not torch.isnan(loss):\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "    \n",
    "    val_loss /= max(val_batches, 1)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Evaluate embeddings quality every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        metrics = evaluate_embeddings(model, train_loader, val_loader, device)\n",
    "        \n",
    "        history['train_separation'].append(metrics['train_separation'])\n",
    "        history['val_separation'].append(metrics['val_separation'])\n",
    "        history['val_auc'].append(metrics['val_auc'])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Train Sep: {metrics['train_separation']:.4f} | Val Sep: {metrics['val_separation']:.4f}\")\n",
    "        print(f\"  Val AUC: {metrics['val_auc']:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        # Save best model\n",
    "        if metrics['val_auc'] > best_val_auc:\n",
    "            best_val_auc = metrics['val_auc']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model based on validation AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Training History Visualization\n\nVisualize three key metrics over training:\n\n1. **Contrastive Loss**: Should decrease and stabilize\n2. **Calinski-Harabasz Index**: Should increase as clusters become better separated\n3. **Validation AUC**: Should improve, indicating better downstream utility\n\nIf CH index increases but AUC plateaus, the model may be overfitting to train-specific patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training history\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Loss curves\naxes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\naxes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Contrastive Loss')\naxes[0].set_title('Training and Validation Loss')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# Calinski-Harabasz Index\neval_epochs = list(range(1, num_epochs+1, 5))\nif 1 not in eval_epochs:\n    eval_epochs = [1] + eval_epochs\naxes[1].plot(eval_epochs[:len(history['train_separation'])], \n             history['train_separation'], marker='o', label='Train CH Index', linewidth=2)\naxes[1].plot(eval_epochs[:len(history['val_separation'])], \n             history['val_separation'], marker='o', label='Val CH Index', linewidth=2)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Calinski-Harabasz Index')\naxes[1].set_title('Calinski-Harabasz Index (Higher is Better)')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\n# AUC\naxes[2].plot(eval_epochs[:len(history['val_auc'])], \n             history['val_auc'], marker='o', color='green', linewidth=2)\naxes[2].axhline(y=0.5, color='red', linestyle='--', label='Random Baseline', alpha=0.5)\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('Validation AUC')\naxes[2].set_title('Logistic Regression AUC on Learned Embeddings')\naxes[2].legend()\naxes[2].grid(alpha=0.3)\naxes[2].set_ylim([0.4, 1.0])\n\nplt.tight_layout()\nplt.savefig('training_history.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Post-Training Embedding Analysis\n\nGenerate the final debiased embeddings by passing all samples through the trained projection network, then compare to the original embeddings.\n\n### What to Look For:\n\n**In the is_positive plots:**\n- ✓ Clearer separation between cases (red) and controls (blue)\n- ✓ Tighter within-class clusters\n\n**In the confounder plots:**\n- ✓ Reduced correlation with confounders (more mixed colors)\n- ✓ Random scatter rather than clear gradients\n\nIf confounders still show strong patterns, consider:\n- Longer training\n- Stronger weight decay\n- Explicit adversarial debiasing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate post-training embeddings for all data\n",
    "model.eval()\n",
    "all_embeddings = torch.FloatTensor(embeddings).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size_inference = 256\n",
    "    trained_embeddings_list = []\n",
    "    \n",
    "    for i in range(0, len(all_embeddings), batch_size_inference):\n",
    "        batch = all_embeddings[i:i+batch_size_inference]\n",
    "        projected = model(batch)\n",
    "        trained_embeddings_list.append(projected.cpu().numpy())\n",
    "    \n",
    "    trained_embeddings = np.vstack(trained_embeddings_list)\n",
    "\n",
    "print(f\"Generated trained embeddings: {trained_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on trained embeddings\n",
    "pca_trained = PCA(n_components=2)\n",
    "pca_coords_trained = pca_trained.fit_transform(trained_embeddings)\n",
    "\n",
    "# Add to dataframe\n",
    "df['PC1_trained'] = pca_coords_trained[:, 0]\n",
    "df['PC2_trained'] = pca_coords_trained[:, 1]\n",
    "\n",
    "print(f\"Trained embeddings - Explained variance ratio: {pca_trained.explained_variance_ratio_}\")\n",
    "print(f\"Trained embeddings - Total variance explained: {pca_trained.explained_variance_ratio_.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs trained embeddings - PC1 vs PC2 colored by is_positive\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original embeddings\n",
    "scatter1 = axes[0].scatter(df['PC1_original'], df['PC2_original'], \n",
    "                          c=df['is_positive'], cmap='coolwarm', \n",
    "                          alpha=0.6, s=20)\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].set_title('Original Embeddings (Before Training)')\n",
    "axes[0].legend(*scatter1.legend_elements(), title=\"is_positive\")\n",
    "\n",
    "# Trained embeddings\n",
    "scatter2 = axes[1].scatter(df['PC1_trained'], df['PC2_trained'], \n",
    "                          c=df['is_positive'], cmap='coolwarm', \n",
    "                          alpha=0.6, s=20)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].set_title('Trained Embeddings (After Contrastive Learning)')\n",
    "axes[1].legend(*scatter2.legend_elements(), title=\"is_positive\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_comparison_is_positive.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-plot comparison: Original vs Trained for each confounder\n",
    "n_confounders = len(confounder_cols) + 1  # +1 for is_positive\n",
    "n_cols = min(3, n_confounders)\n",
    "n_rows = (n_confounders + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows * 2, n_cols, figsize=(6*n_cols, 5*n_rows*2))\n",
    "axes = axes.flatten() if n_confounders > 1 else [axes]\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "# Plot is_positive first\n",
    "for emb_type, pc1_col, pc2_col, title_suffix in [\n",
    "    ('original', 'PC1_original', 'PC2_original', 'Original'),\n",
    "    ('trained', 'PC1_trained', 'PC2_trained', 'Trained')\n",
    "]:\n",
    "    scatter = axes[plot_idx].scatter(df[pc1_col], df[pc2_col], \n",
    "                                    c=df['is_positive'], cmap='coolwarm', \n",
    "                                    alpha=0.6, s=20)\n",
    "    axes[plot_idx].set_xlabel('PC1')\n",
    "    axes[plot_idx].set_ylabel('PC2')\n",
    "    axes[plot_idx].set_title(f'PC1 vs PC2 ({title_suffix}) - is_positive')\n",
    "    axes[plot_idx].legend(*scatter.legend_elements(), title=\"is_positive\", fontsize=8)\n",
    "    plot_idx += 1\n",
    "\n",
    "# Plot each confounder\n",
    "for confounder in confounder_cols:\n",
    "    for emb_type, pc1_col, pc2_col, title_suffix in [\n",
    "        ('original', 'PC1_original', 'PC2_original', 'Original'),\n",
    "        ('trained', 'PC1_trained', 'PC2_trained', 'Trained')\n",
    "    ]:\n",
    "        if plot_idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Check if confounder is categorical or continuous\n",
    "        if df[confounder].dtype == 'object' or df[confounder].nunique() < 10:\n",
    "            # Categorical\n",
    "            scatter = axes[plot_idx].scatter(df[pc1_col], df[pc2_col], \n",
    "                                           c=pd.Categorical(df[confounder]).codes, \n",
    "                                           cmap='tab10', alpha=0.6, s=20)\n",
    "            axes[plot_idx].legend(*scatter.legend_elements(), title=confounder, \n",
    "                                loc='best', fontsize=8)\n",
    "        else:\n",
    "            # Continuous\n",
    "            scatter = axes[plot_idx].scatter(df[pc1_col], df[pc2_col], \n",
    "                                           c=df[confounder], cmap='viridis', \n",
    "                                           alpha=0.6, s=20)\n",
    "            plt.colorbar(scatter, ax=axes[plot_idx], label=confounder)\n",
    "        \n",
    "        axes[plot_idx].set_xlabel('PC1')\n",
    "        axes[plot_idx].set_ylabel('PC2')\n",
    "        axes[plot_idx].set_title(f'PC1 vs PC2 ({title_suffix}) - {confounder}')\n",
    "        plot_idx += 1\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(plot_idx, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_comparison_all_confounders.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete! Check the saved PNG files for detailed comparisons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Summary and Export\n\n### Final Metrics\nQuantify the improvement achieved by contrastive learning:\n- **Cluster separation improvement**: How much better are cases/controls separated?\n- **AUC improvement**: Is the embedding more useful for downstream prediction?\n\n### Saving Results\nOptionally save:\n1. **Trained embeddings**: For use in downstream analyses (GWAS, prediction models, etc.)\n2. **Model weights**: To apply the same transformation to new samples\n\n### Next Steps\nWith the debiased embeddings, you can:\n- Run association studies with reduced confounding\n- Train fairer predictive models\n- Perform clustering or dimensionality reduction with less technical artifact"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final evaluation on all data\nprint(\"=\" * 60)\nprint(\"FINAL SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"\\nBest validation AUC achieved: {best_val_auc:.4f}\")\nprint(f\"\\nOriginal embeddings:\")\nprint(f\"  Shape: {embeddings.shape}\")\nprint(f\"  PCA variance explained (PC1+PC2): {pca_original.explained_variance_ratio_.sum():.3f}\")\n\nprint(f\"\\nTrained embeddings:\")\nprint(f\"  Shape: {trained_embeddings.shape}\")\nprint(f\"  PCA variance explained (PC1+PC2): {pca_trained.explained_variance_ratio_.sum():.3f}\")\n\n# Compute final metrics on full dataset\nfinal_ch_original = compute_cluster_separation(embeddings, df['is_positive'].values)\nfinal_ch_trained = compute_cluster_separation(trained_embeddings, df['is_positive'].values)\n\nprint(f\"\\nCalinski-Harabasz Index (higher is better):\")\nprint(f\"  Original embeddings: {final_ch_original:.2f}\")\nprint(f\"  Trained embeddings: {final_ch_trained:.2f}\")\nprint(f\"  Improvement: {((final_ch_trained - final_ch_original) / final_ch_original * 100):.2f}%\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Files saved:\")\nprint(\"  - pca_original_confounders.png\")\nprint(\"  - training_history.png\")\nprint(\"  - pca_comparison_is_positive.png\")\nprint(\"  - pca_comparison_all_confounders.png\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save trained embeddings and model\n",
    "# Uncomment to save\n",
    "\n",
    "# # Save trained embeddings to parquet\n",
    "# df_export = df.copy()\n",
    "# for i in range(trained_embeddings.shape[1]):\n",
    "#     df_export[f'trained_emb_{i}'] = trained_embeddings[:, i]\n",
    "# df_export.to_parquet('embeddings_with_trained.parquet', index=False)\n",
    "# print(\"Saved embeddings with trained representations to 'embeddings_with_trained.parquet'\")\n",
    "\n",
    "# # Save model weights\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'input_dim': input_dim,\n",
    "#     'hidden_dim': 256,\n",
    "#     'output_dim': 128,\n",
    "#     'best_val_auc': best_val_auc\n",
    "# }, 'contrastive_model.pt')\n",
    "# print(\"Saved model weights to 'contrastive_model.pt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}