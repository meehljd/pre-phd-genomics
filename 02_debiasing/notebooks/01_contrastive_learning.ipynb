{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Learning for Debiasing Genomic Embeddings\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements supervised contrastive learning to improve pre-computed genomic embeddings by reducing confounding effects from ancestry and technical variables while preserving discriminative signal for the outcome of interest.\n",
    "\n",
    "## Background\n",
    "\n",
    "Genomic data often contains confounding factors (ancestry, sequencing batch, read depth, etc.) that can lead to spurious associations in downstream analyses. Traditional approaches like linear regression adjustment or propensity score matching have limitations. Contrastive learning offers an alternative: learn an embedding space where:\n",
    "\n",
    "1. **Samples with the same phenotype** are pulled together (positive pairs)\n",
    "2. **Samples with different phenotypes** but matched confounders are pushed apart (negative pairs)\n",
    "3. **Confounder effects** are minimized through the matched design\n",
    "\n",
    "## Study Design\n",
    "\n",
    "We use a **matched case-control design** with:\n",
    "- Cases: Samples with `is_positive = 1`\n",
    "- Controls: 4 matched controls per case (matched on confounders)\n",
    "- Each case-control group is identified by `case_matched` column\n",
    "\n",
    "### Contrastive Pairs Definition\n",
    "\n",
    "- **Positive pairs**: Two samples with the same `is_positive` label AND same `case_matched` group\n",
    "- **Negative pairs**: Two samples with different `is_positive` labels AND same `case_matched` group\n",
    "\n",
    "This ensures we're learning to discriminate between cases and controls while being invariant to the confounders they share.\n",
    "\n",
    "## Training Strategy\n",
    "\n",
    "1. Split data by `case_matched` groups (20% validation) to prevent leakage\n",
    "2. Train a projection network using supervised contrastive loss\n",
    "3. Monitor progress using cluster separation and logistic regression AUC\n",
    "4. Compare embeddings before and after training using PCA visualizations\n",
    "\n",
    "## Expected Outcome\n",
    "\n",
    "The trained embeddings should:\n",
    "- ✓ Improve separation between cases and controls (higher AUC)\n",
    "- ✓ Reduce correlation with confounding variables\n",
    "- ✓ Maintain or improve downstream predictive performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, calinski_harabasz_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load the parquet file containing pre-computed embeddings and metadata. The data should include:\n",
    "- **Embeddings**: Pre-computed feature vectors (e.g., from a variant autoencoder, PRS, or other genomic model)\n",
    "- **is_positive**: Binary outcome label (1 = case, 0 = control)\n",
    "- **case_matched**: Group ID linking each case to its matched controls\n",
    "- **Confounders**: Variables to control for (ancestry, batch, read depth, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_REAL_DATA = False\n",
    "\n",
    "if USE_REAL_DATA:\n",
    "    EMBEDDING_COLS = [\n",
    "        \"SIRPG\",\n",
    "        \"BACH2\",\n",
    "    ]\n",
    "    MATCH_CASE_COL = \"matched_case_kitid\"\n",
    "else:\n",
    "    EMBEDDING_COLS = [\n",
    "        \"α-syn\",\n",
    "        \"β-k3\",\n",
    "        \"γ-DR\",\n",
    "        \"ε-BP\",\n",
    "        \"λ-trans7\",\n",
    "        \"ζ-lig\",\n",
    "        \"ω-deg\",\n",
    "        \"κ-pol\",\n",
    "        \"η-rec\",\n",
    "        \"δ-act2\",\n",
    "        \"θ-phos\",\n",
    "        \"μ-chan\",\n",
    "        \"ξ-meth\",\n",
    "        \"π-ox\",\n",
    "        \"ρ-hydr\",\n",
    "        \"σ-cat\",\n",
    "        \"τ-tub\",\n",
    "        \"υ-reg\",\n",
    "        \"φ-fold\",\n",
    "        \"χ-chap\",\n",
    "        \"ψ-sens\",\n",
    "    ][:5]\n",
    "    MATCH_CASE_COL = \"case_id\"\n",
    "\n",
    "LABEL_COL = \"is_positive\"\n",
    "\n",
    "data_path = \"/home/ext_meehl_joshua_mayo_edu/strand_cohort_eda/genomic/evals/data/embeddings/t1d/t1d_v1_emb_matrix_named.parquet\"\n",
    "meta_path = \"/home/ext_meehl_joshua_mayo_edu/strand_cohort_eda/genomic/evals/data/gsm/t1d/matched_has_diabetes1_v4_modified.csv\"\n",
    "cofound_path = \"/home/ext_meehl_joshua_mayo_edu/strand_cohort_eda/genomic/datasets/golden_benchmark/data/cohort_design/master_cohort.csv\"\n",
    "\n",
    "synth_path = \"/root/pre-phd-genomics/02_debiasing/data/synthetic_embeddings.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_REAL_DATA:\n",
    "    df_emb = pd.read_parquet(data_path)\n",
    "    print(df_emb.shape)\n",
    "\n",
    "    df_meta = pd.read_csv(meta_path)\n",
    "    df_meta[\"sample_id\"] = df_meta[\"tap_kitid\"]\n",
    "    mask = df_meta[\"sample_id\"].isin(df_emb[\"sample_id\"].unique())\n",
    "    df_meta = df_meta[mask]\n",
    "    print(df_meta.shape)\n",
    "    # df_meta.head(10)\n",
    "\n",
    "    df_conf = pd.read_csv(cofound_path)\n",
    "    df_conf[\"sample_id\"] = df_conf[\"tap_kitid\"]\n",
    "    mask = df_conf[\"sample_id\"].isin(df_emb[\"sample_id\"].unique())\n",
    "    df_conf = df_conf[mask]\n",
    "    print(df_conf.shape)\n",
    "    # df_conf.head(10)\n",
    "\n",
    "    cols = [\"sample_id\"] + EMBEDDING_COLS\n",
    "    df = df_emb[cols].copy()\n",
    "    df = df.merge(df_meta, on=\"sample_id\", how=\"left\")\n",
    "    df = df.merge(df_conf, on=\"sample_id\", how=\"left\")\n",
    "else:\n",
    "    df = pd.read_parquet(synth_path)\n",
    "    print(df.shape)\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embedding columns\n",
    "# Each column contains a numpy array of shape (gene_dim,)\n",
    "print(f\"Found {len(EMBEDDING_COLS)} gene embedding columns: {EMBEDDING_COLS}\")\n",
    "\n",
    "# Extract embeddings properly from dataframe\n",
    "# Each gene column contains arrays, we need to stack them into (n_samples, gene_dim, n_genes)\n",
    "n_samples = len(df)\n",
    "n_genes = len(EMBEDDING_COLS)\n",
    "\n",
    "# Get gene_dim from first array\n",
    "GENE_DIM = df[EMBEDDING_COLS[0]].iloc[0].shape[0]\n",
    "print(f\"Gene dimension: {GENE_DIM}\")\n",
    "\n",
    "# Create 3D tensor: (n_samples, gene_dim, n_genes)\n",
    "embeddings = np.zeros((n_samples, GENE_DIM, n_genes))\n",
    "\n",
    "for gene_idx, gene_col in enumerate(EMBEDDING_COLS):\n",
    "    # Stack all arrays for this gene\n",
    "    embeddings[:, :, gene_idx] = np.vstack(df[gene_col].values)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape} (n_samples, gene_dim, n_genes)\")\n",
    "print(f\"  - {n_samples} samples\")\n",
    "print(f\"  - {GENE_DIM} features per gene\")\n",
    "print(f\"  - {n_genes} genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the embeddings shape\n",
    "print(f\"Embeddings successfully reshaped to: {embeddings.shape}\")\n",
    "print(f\"First sample, first gene, first 5 features: {embeddings[0, :5, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings are now ready for the genewise contrastive learning architecture\n",
    "# Shape: (n_samples, gene_dim, n_genes) = ({embeddings.shape[0]}, {embeddings.shape[1]}, {embeddings.shape[2]})\n",
    "print(f\"✓ Embeddings ready for genewise architecture\")\n",
    "print(f\"  Total features: {GENE_DIM * n_genes}\")\n",
    "print(f\"  Format: Each of {n_genes} genes has {GENE_DIM} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on original embeddings (flatten for PCA)\n",
    "embeddings_flat = embeddings.reshape(embeddings.shape[0], -1)  # (n, gene_dim * n_genes)\n",
    "pca_original = PCA(n_components=2)\n",
    "pca_coords_original = pca_original.fit_transform(embeddings_flat)\n",
    "\n",
    "# Add PCA coordinates to dataframe\n",
    "df[\"PC1_original\"] = pca_coords_original[:, 0]\n",
    "df[\"PC2_original\"] = pca_coords_original[:, 1]\n",
    "\n",
    "print(f\"Explained variance ratio: {pca_original.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca_original.explained_variance_ratio_.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique case_matched groups (each represents a case + its 4 matched controls)\n",
    "unique_groups = df[\"case_matched\"].unique()\n",
    "print(f\"Total unique case groups: {len(unique_groups)}\")\n",
    "\n",
    "# Split groups into train/val (20% validation)\n",
    "train_groups, val_groups = train_test_split(\n",
    "    unique_groups, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Train groups: {len(train_groups)}, Val groups: {len(val_groups)}\")\n",
    "\n",
    "# Create train and validation masks\n",
    "train_mask = df[\"case_matched\"].isin(train_groups)\n",
    "val_mask = df[\"case_matched\"].isin(val_groups)\n",
    "\n",
    "df_train = df[train_mask].copy()\n",
    "df_val = df[val_mask].copy()\n",
    "\n",
    "print(f\"\\nTrain set: {len(df_train)} samples ({df_train[LABEL_COL].sum()} positive)\")\n",
    "print(f\"Val set: {len(df_val)} samples ({df_val[LABEL_COL].sum()} positive)\")\n",
    "\n",
    "# Extract embeddings for train and val (already reshaped to 3D)\n",
    "X_train = embeddings[train_mask]  # (n_train, gene_dim, n_genes)\n",
    "y_train = df_train[LABEL_COL].values\n",
    "X_val = embeddings[val_mask]  # (n_val, gene_dim, n_genes)\n",
    "y_val = df_val[LABEL_COL].values\n",
    "\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Contrastive Learning Architecture\n",
    "\n",
    "### GenewiseContrastiveProjector\n",
    "A gene-aware projection network that works with **any number of genes**:\n",
    "- **Input**: (batch_size, gene_dim, n_genes) where gene_dim is typically 1024\n",
    "- **Shared gene projection**: Applies same transformation to each gene's features\n",
    "- **Pooling**: Aggregates across genes (mean pooling or attention)\n",
    "- **Final projection**: Maps pooled features to output space\n",
    "- **L2 Normalization**: Output embeddings lie on unit hypersphere\n",
    "\n",
    "**Key advantage**: Train once, works for any gene set (100 genes, 500 genes, etc.)\n",
    "\n",
    "### ContrastiveLoss\n",
    "Supervised contrastive loss adapted for matched case-control design:\n",
    "- Uses temperature-scaled cosine similarity\n",
    "- Only considers pairs within the same `case_matched` group\n",
    "- Pulls together samples with same label, pushes apart samples with different labels\n",
    "- Automatically handles variable numbers of positive pairs per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenewiseContrastiveProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Gene-aware projection network for contrastive learning.\n",
    "    Works with any number of genes by applying shared projection per gene,\n",
    "    then pooling across genes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, gene_dim=1024, hidden_dim=256, output_dim=128, dropout=0.1, pooling=\"mean\"\n",
    "    ):\n",
    "        super(GenewiseContrastiveProjector, self).__init__()\n",
    "\n",
    "        self.pooling = pooling\n",
    "\n",
    "        # Shared projection applied to each gene independently\n",
    "        self.gene_projector = nn.Sequential(\n",
    "            nn.Linear(gene_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Attention pooling (optional)\n",
    "        if pooling == \"attention\":\n",
    "            self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Final projection after pooling\n",
    "        self.final_projector = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (batch_size, gene_dim, num_genes)\n",
    "        Returns:\n",
    "            embeddings: shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size, gene_dim, num_genes = x.shape\n",
    "\n",
    "        # Reshape to (batch_size * num_genes, gene_dim)\n",
    "        x = x.permute(0, 2, 1)  # (batch, genes, features)\n",
    "        x = x.reshape(batch_size * num_genes, gene_dim)\n",
    "\n",
    "        # Apply shared gene projection\n",
    "        x = self.gene_projector(x)  # (batch*genes, hidden_dim)\n",
    "\n",
    "        # Reshape back to (batch_size, num_genes, hidden_dim)\n",
    "        x = x.reshape(batch_size, num_genes, -1)\n",
    "\n",
    "        # Pool across genes\n",
    "        if self.pooling == \"mean\":\n",
    "            x = x.mean(dim=1)  # (batch, hidden_dim)\n",
    "        elif self.pooling == \"max\":\n",
    "            x = x.max(dim=1)[0]  # (batch, hidden_dim)\n",
    "        elif self.pooling == \"attention\":\n",
    "            weights = torch.softmax(self.attention(x), dim=1)  # (batch, genes, 1)\n",
    "            x = (x * weights).sum(dim=1)  # (batch, hidden_dim)\n",
    "\n",
    "        # Final projection\n",
    "        x = self.final_projector(x)  # (batch, output_dim)\n",
    "\n",
    "        return F.normalize(x, dim=1)\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised contrastive loss for matched case-control design.\n",
    "    Positive pairs: same is_positive label AND same case_matched group\n",
    "    Negative pairs: different is_positive label AND same case_matched group (matched controls)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels, case_matched):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: normalized embeddings, shape (batch_size, embed_dim)\n",
    "            labels: is_positive labels, shape (batch_size,)\n",
    "            case_matched: case_matched group IDs, shape (batch_size,)\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        batch_size = features.shape[0]\n",
    "\n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "\n",
    "        # Create masks for positive and negative pairs\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        case_matched = case_matched.contiguous().view(-1, 1)\n",
    "\n",
    "        # Positive mask: same label AND same case_matched group (but not same sample)\n",
    "        label_mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        case_mask = torch.eq(case_matched, case_matched.T).float().to(device)\n",
    "        positive_mask = label_mask * case_mask\n",
    "        positive_mask.fill_diagonal_(0)  # Exclude self-comparisons\n",
    "\n",
    "        # Negative mask: different label AND same case_matched group\n",
    "        negative_mask = (1 - label_mask) * case_mask\n",
    "\n",
    "        # For numerical stability\n",
    "        logits_max, _ = torch.max(similarity_matrix, dim=1, keepdim=True)\n",
    "        logits = similarity_matrix - logits_max.detach()\n",
    "\n",
    "        # Compute log probabilities\n",
    "        exp_logits = torch.exp(logits) * (\n",
    "            1 - torch.eye(batch_size).to(device)\n",
    "        )  # Exclude diagonal\n",
    "\n",
    "        # Only consider negatives from same case_matched group\n",
    "        log_prob = logits - torch.log(\n",
    "            torch.sum(\n",
    "                exp_logits * (positive_mask + negative_mask + 1e-8), dim=1, keepdim=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Compute mean of log-likelihood over positive pairs\n",
    "        mean_log_prob_pos = (positive_mask * log_prob).sum(1) / (\n",
    "            positive_mask.sum(1) + 1e-8\n",
    "        )\n",
    "\n",
    "        # Loss is negative log-likelihood\n",
    "        loss = -mean_log_prob_pos\n",
    "        loss = loss[\n",
    "            positive_mask.sum(1) > 0\n",
    "        ].mean()  # Only compute loss for samples with positive pairs\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Initialize model - works for ANY number of genes!\n",
    "gene_dim = GENE_DIM  # From the embeddings we extracted\n",
    "n_genes_model = n_genes  # Number of genes\n",
    "print(f\"Gene dimension: {gene_dim}\")\n",
    "print(f\"Number of genes: {n_genes_model}\")\n",
    "print(\n",
    "    f\"Building genewise model with gene_dim={gene_dim}, hidden_dim=256, output_dim=128\"\n",
    ")\n",
    "\n",
    "model = GenewiseContrastiveProjector(\n",
    "    gene_dim=gene_dim,\n",
    "    hidden_dim=256,\n",
    "    output_dim=128,\n",
    "    pooling=\"mean\",  # Options: 'mean', 'max', 'attention'\n",
    ").to(device)\n",
    "\n",
    "criterion = ContrastiveLoss(temperature=0.07)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nThis model works for ANY number of genes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell - duplicate PCA already done in cell-8\n",
    "# PCA will be recomputed on the filtered dataset in cell-37 after training\n",
    "print(\"PCA will be computed on filtered data in cell-37 after training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This PCA is on the FULL dataset before filtering in cell-18\n",
    "# This is just for initial exploration\n",
    "# The final PCA comparisons will be done in cell-37 after training\n",
    "\n",
    "# Perform PCA on original embeddings (full dataset, before filtering)\n",
    "pca_original_full = PCA(n_components=2)\n",
    "embeddings_flat_full = embeddings.reshape(embeddings.shape[0], -1)\n",
    "pca_coords_original_full = pca_original_full.fit_transform(embeddings_flat_full)\n",
    "\n",
    "print(f\"PCA on full dataset (before filtering in cell-18):\")\n",
    "print(f\"  Explained variance ratio: {pca_original_full.explained_variance_ratio_}\")\n",
    "print(\n",
    "    f\"  Total variance explained: {pca_original_full.explained_variance_ratio_.sum():.3f}\"\n",
    ")\n",
    "print(f\"\\nNote: Final PCA for visualization will be computed in cell-37 after training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify confounder columns (UPDATE THESE based on your data)\n",
    "# Examples: ancestry, batch, read_depth, sequencing_platform, etc.\n",
    "confounder_cols = [\n",
    "    col\n",
    "    for col in df.columns\n",
    "    if col\n",
    "    in [\n",
    "        \"has_t1d\",\n",
    "        \"is_european\",\n",
    "        \"is_gt_65_years\",\n",
    "        \"vcf_assay_version\",\n",
    "        \"is_female\",\n",
    "        \"is_bmi_gt_30\",  # Add your actual confounder column names\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(f\"Confounder columns found: {confounder_cols}\")\n",
    "if len(confounder_cols) == 0:\n",
    "    print(\n",
    "        \"WARNING: No confounder columns found. Please update the confounder_cols list above.\"\n",
    "    )\n",
    "    # Create dummy example for demonstration\n",
    "    confounder_cols = [LABEL_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-plot visualization of PC1 and PC2 vs confounders\n",
    "show_plt = True\n",
    "\n",
    "if show_plt:\n",
    "\n",
    "    from matplotlib.colors import ListedColormap\n",
    "\n",
    "    n_confounders = len(confounder_cols) + 1  # +1 for the main is_positive plot\n",
    "    n_cols = min(3, n_confounders)\n",
    "    n_rows = (n_confounders + n_cols - 1) // n_cols\n",
    "\n",
    "    # Use more contrasting colormaps\n",
    "    contrast_cmap = ListedColormap(\n",
    "        [\n",
    "            \"#e41a1c\",\n",
    "            \"#ffff33\",\n",
    "            \"#377eb8\",\n",
    "        ]\n",
    "    )  # \"#4daf4a\", \"#984ea3\", \"#ff7f00\", \"#a65628\", \"#f781bf\", \"#999999\"])\n",
    "    contrast_cmap_cont = \"plasma\"  # For continuous variables\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 5 * n_rows))\n",
    "    axes = axes.flatten() if n_confounders > 1 else [axes]\n",
    "\n",
    "    # Plot PC1 vs PC2 colored by is_positive (main outcome)\n",
    "    scatter = axes[0].scatter(\n",
    "        df[\"PC1_original\"],\n",
    "        df[\"PC2_original\"],\n",
    "        c=df[LABEL_COL],\n",
    "        cmap=contrast_cmap,\n",
    "        alpha=0.7,\n",
    "        s=30,\n",
    "        edgecolor=\"k\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axes[0].set_xlabel(\"PC1\")\n",
    "    axes[0].set_ylabel(\"PC2\")\n",
    "    axes[0].set_title(\"PC1 vs PC2 (Original) - Colored by is_positive\")\n",
    "    axes[0].legend(*scatter.legend_elements(), title=\"is_positive\")\n",
    "\n",
    "    # Plot PC1 vs PC2 colored by each confounder\n",
    "    for idx, confounder in enumerate(confounder_cols, start=1):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "\n",
    "        # Check if confounder is categorical or continuous\n",
    "        if df[confounder].dtype == \"object\" or df[confounder].nunique() < 10:\n",
    "            # Categorical - use discrete, high-contrast colors\n",
    "            scatter = axes[idx].scatter(\n",
    "                df[\"PC1_original\"],\n",
    "                df[\"PC2_original\"],\n",
    "                c=pd.Categorical(df[confounder]).codes,\n",
    "                cmap=contrast_cmap,\n",
    "                alpha=0.25,\n",
    "                s=30,\n",
    "                edgecolor=\"k\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "            axes[idx].legend(\n",
    "                *scatter.legend_elements(), title=confounder, loc=\"best\", fontsize=8\n",
    "            )\n",
    "        else:\n",
    "            # Continuous - use a more vibrant colormap\n",
    "            scatter = axes[idx].scatter(\n",
    "                df[\"PC1_original\"],\n",
    "                df[\"PC2_original\"],\n",
    "                c=df[confounder],\n",
    "                cmap=contrast_cmap_cont,\n",
    "                alpha=0.25,\n",
    "                s=30,\n",
    "                edgecolor=\"k\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "            plt.colorbar(scatter, ax=axes[idx], label=confounder)\n",
    "\n",
    "        axes[idx].set_xlabel(\"PC1\")\n",
    "        axes[idx].set_ylabel(\"PC2\")\n",
    "        axes[idx].set_title(f\"PC1 vs PC2 (Original) - Colored by {confounder}\")\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_confounders, len(axes)):\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"pca_original_confounders.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Validation Split\n",
    "\n",
    "**Critical**: We split by `case_matched` groups (not individual samples) to prevent data leakage.\n",
    "\n",
    "If we randomly split individuals, a case and its matched controls might end up in different sets, allowing the model to \"cheat\" by learning confounder patterns that appear in both train and validation.\n",
    "\n",
    "By keeping each case-control group together, we ensure the model generalizes to unseen confounder combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_only_mask = df[\"split\"] == \"train\"\n",
    "df = df[train_only_mask]\n",
    "\n",
    "missing_case_mask = df[MATCH_CASE_COL].isna()\n",
    "df.loc[missing_case_mask, MATCH_CASE_COL] = df.loc[missing_case_mask, \"sample_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate train/val splits by matched_case_kitid groups\n",
    "unique_groups = df[MATCH_CASE_COL].unique()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_groups, val_groups = train_test_split(\n",
    "    unique_groups, test_size=0.2, random_state=42\n",
    ")\n",
    "train_mask = df[MATCH_CASE_COL].isin(train_groups)\n",
    "val_mask = df[MATCH_CASE_COL].isin(val_groups)\n",
    "df_train = df[train_mask].copy()\n",
    "df_val = df[val_mask].copy()\n",
    "print(f\"Train set: {len(df_train)} samples ({df_train[LABEL_COL].sum()} positive)\")\n",
    "print(f\"Val set: {len(df_val)} samples ({df_val[LABEL_COL].sum()} positive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(df, EMBEDDING_COLS):\n",
    "    \"\"\"\n",
    "    Extract embeddings from dataframe where each gene column contains numpy arrays.\n",
    "    Returns a 3D array of shape (n_samples, gene_dim, n_genes).\n",
    "    \"\"\"\n",
    "    n_samples = len(df)\n",
    "    n_genes = len(EMBEDDING_COLS)\n",
    "\n",
    "    # Get gene_dim from first array\n",
    "    gene_dim = df[EMBEDDING_COLS[0]].iloc[0].shape[0]\n",
    "\n",
    "    # Create 3D tensor: (n_samples, gene_dim, n_genes)\n",
    "    embeddings = np.zeros((n_samples, gene_dim, n_genes))\n",
    "\n",
    "    for gene_idx, gene_col in enumerate(EMBEDDING_COLS):\n",
    "        # Stack all arrays for this gene\n",
    "        embeddings[:, :, gene_idx] = np.vstack(df[gene_col].values)\n",
    "\n",
    "    return embeddings.astype(np.float32)\n",
    "\n",
    "\n",
    "X_train = extract_embeddings(df_train, EMBEDDING_COLS)\n",
    "y_train = df_train[LABEL_COL].values\n",
    "case_matched_train = pd.factorize(df_train[MATCH_CASE_COL])[0]\n",
    "\n",
    "X_val = extract_embeddings(df_val, EMBEDDING_COLS)\n",
    "y_val = df_val[LABEL_COL].values\n",
    "case_matched_val = pd.factorize(df_val[MATCH_CASE_COL])[0]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape} (n_samples, gene_dim, n_genes)\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"Ready for genewise architecture!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EmbeddingDataset class\n",
    "class EmbeddingDataset(Dataset):\n",
    "    \"\"\"Dataset for pre-computed embeddings with labels and case_matched groups\"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, labels, case_matched):\n",
    "        # embeddings shape: (n_samples, gene_dim, n_genes)\n",
    "        self.embeddings = torch.FloatTensor(embeddings)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.case_matched = torch.LongTensor(case_matched)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx], self.case_matched[idx]\n",
    "\n",
    "\n",
    "# Check if each factor in case_matched_train has exactly 5 samples\n",
    "import collections\n",
    "\n",
    "factor_counts = collections.Counter(case_matched_train)\n",
    "print(\"Value counts for each factor in case_matched_train:\")\n",
    "print(f\"Total unique groups: {len(factor_counts)}\")\n",
    "print(f\"Min samples per group: {min(factor_counts.values())}\")\n",
    "print(f\"Max samples per group: {max(factor_counts.values())}\")\n",
    "all_five = all(count == 5 for count in factor_counts.values())\n",
    "print(f\"All factors have exactly 5 samples? {all_five}\")\n",
    "\n",
    "# Create datasets and dataloaders for genewise architecture\n",
    "print(f\"\\nCreating datasets for genewise architecture...\")\n",
    "train_dataset = EmbeddingDataset(X_train, y_train, case_matched_train)\n",
    "val_dataset = EmbeddingDataset(X_val, y_val, case_matched_val)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataLoaders created successfully!\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(\n",
    "    f\"  Batch shape: (batch_size={batch_size}, gene_dim={X_train.shape[1]}, n_genes={X_train.shape[2]})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Contrastive Learning Architecture\n",
    "\n",
    "### ContrastiveProjector\n",
    "A 3-layer MLP that projects the input embeddings to a lower-dimensional space (128D) optimized for contrastive learning:\n",
    "- **BatchNorm**: Stabilizes training and prevents internal covariate shift\n",
    "- **Dropout**: Prevents overfitting to specific confounder patterns\n",
    "- **L2 Normalization**: Output embeddings lie on unit hypersphere, making cosine similarity meaningful\n",
    "\n",
    "### ContrastiveLoss\n",
    "Supervised contrastive loss adapted for matched case-control design:\n",
    "- Uses temperature-scaled cosine similarity\n",
    "- Only considers pairs within the same `case_matched` group\n",
    "- Pulls together samples with same label, pushes apart samples with different labels\n",
    "- Automatically handles variable numbers of positive pairs per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and loss already defined in cell-11 (GenewiseContrastiveProjector)\n",
    "# This cell is skipped to avoid overwriting the genewise model\n",
    "print(\"Using GenewiseContrastiveProjector model from cell-11\")\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset and DataLoader\n",
    "\n",
    "PyTorch dataset wrapper for the embeddings with custom collation to pass:\n",
    "1. Pre-computed embeddings (input features)\n",
    "2. Labels (`is_positive`)\n",
    "3. Case-match group IDs (for contrastive loss computation)\n",
    "\n",
    "The DataLoader shuffles training data while keeping batches reasonably sized to ensure diverse case-control groups per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell - embeddings will be generated after training in cell-36\n",
    "print(\"Skipping - embeddings will be generated after training (see cell-36)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitoring Functions\n",
    "\n",
    "We track two complementary metrics during training:\n",
    "\n",
    "### 1. Calinski-Harabasz Index (Geometric)\n",
    "Ratio of between-cluster variance to within-cluster variance. Higher values indicate better-defined, more separated clusters.\n",
    "\n",
    "### 2. Logistic Regression AUC (Discriminative)\n",
    "Train a simple linear classifier on the embeddings and evaluate on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_separation(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Compute Calinski-Harabasz Index (Variance Ratio Criterion).\n",
    "    Measures the ratio of between-cluster to within-cluster variance.\n",
    "    Higher values indicate better-defined clusters.\n",
    "\n",
    "    Returns:\n",
    "        float: CH index score (higher is better, range [0, inf))\n",
    "    \"\"\"\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    return calinski_harabasz_score(embeddings, labels)\n",
    "\n",
    "\n",
    "def compute_logistic_auc(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier and return validation AUC.\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_proba = clf.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "    return auc\n",
    "\n",
    "\n",
    "def evaluate_embeddings(model, train_loader, val_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of learned embeddings using cluster separation and AUC.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Get all train embeddings\n",
    "    train_embeddings_list = []\n",
    "    train_labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels, _ in train_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            projected = model(embeddings)\n",
    "            train_embeddings_list.append(projected.cpu().numpy())\n",
    "            train_labels_list.append(labels.numpy())\n",
    "\n",
    "    train_embeddings = np.vstack(train_embeddings_list)\n",
    "    train_labels = np.concatenate(train_labels_list)\n",
    "\n",
    "    # Get all val embeddings\n",
    "    val_embeddings_list = []\n",
    "    val_labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels, _ in val_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            projected = model(embeddings)\n",
    "            val_embeddings_list.append(projected.cpu().numpy())\n",
    "            val_labels_list.append(labels.numpy())\n",
    "\n",
    "    val_embeddings = np.vstack(val_embeddings_list)\n",
    "    val_labels = np.concatenate(val_labels_list)\n",
    "\n",
    "    # Compute metrics\n",
    "    train_separation = compute_cluster_separation(train_embeddings, train_labels)\n",
    "    val_separation = compute_cluster_separation(val_embeddings, val_labels)\n",
    "    auc = compute_logistic_auc(\n",
    "        train_embeddings, train_labels, val_embeddings, val_labels\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train_separation\": train_separation,\n",
    "        \"val_separation\": val_separation,\n",
    "        \"val_auc\": auc,\n",
    "        \"train_embeddings\": train_embeddings,\n",
    "        \"train_labels\": train_labels,\n",
    "        \"val_embeddings\": val_embeddings,\n",
    "        \"val_labels\": val_labels,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Monitoring functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "Train the projection network with:\n",
    "- **AdamW optimizer**: Decoupled weight decay for better generalization\n",
    "- **Cosine annealing schedule**: Gradually reduces learning rate for fine-tuning\n",
    "- **Evaluation every 5 epochs**: Track cluster separation and AUC without slowing training\n",
    "- **Best model selection**: Save model with highest validation AUC\n",
    "\n",
    "**Note**: Some batches may have NaN loss if they don't contain valid positive pairs. These are safely skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_separation\": [],\n",
    "    \"val_separation\": [],\n",
    "    \"val_auc\": [],\n",
    "}\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs...\")\n",
    "print(f\"Initial evaluation before training:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "best_val_auc = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_batches = 0\n",
    "\n",
    "    for embeddings, labels, case_matched in tqdm(\n",
    "        train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"\n",
    "    ):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        case_matched = case_matched.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        projected = model(embeddings)\n",
    "        loss = criterion(projected, labels, case_matched)\n",
    "\n",
    "        # Skip if loss is nan (can happen if batch has no valid pairs)\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    train_loss /= max(train_batches, 1)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels, case_matched in val_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "            case_matched = case_matched.to(device)\n",
    "\n",
    "            projected = model(embeddings)\n",
    "            loss = criterion(projected, labels, case_matched)\n",
    "\n",
    "            if not torch.isnan(loss):\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "    val_loss /= max(val_batches, 1)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate embeddings quality every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        metrics = evaluate_embeddings(model, train_loader, val_loader, device)\n",
    "\n",
    "        history[\"train_separation\"].append(metrics[\"train_separation\"])\n",
    "        history[\"val_separation\"].append(metrics[\"val_separation\"])\n",
    "        history[\"val_auc\"].append(metrics[\"val_auc\"])\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(\n",
    "            f\"  Train Sep: {metrics['train_separation']:.4f} | Val Sep: {metrics['val_separation']:.4f}\"\n",
    "        )\n",
    "        print(f\"  Val AUC: {metrics['val_auc']:.4f}\")\n",
    "        print()\n",
    "\n",
    "        # Save best model\n",
    "        if metrics[\"val_auc\"] > best_val_auc:\n",
    "            best_val_auc = metrics[\"val_auc\"]\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This summary cell should come AFTER training\n",
    "# Moving summary to cell-41 where it belongs\n",
    "print(\"Summary will be displayed after training completes (see cell-41)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model based on validation AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training History Visualization\n",
    "\n",
    "Visualize three key metrics over training:\n",
    "\n",
    "1. **Contrastive Loss**: Should decrease and stabilize\n",
    "2. **Calinski-Harabasz Index**: Should increase as clusters become better separated\n",
    "3. **Validation AUC**: Should improve, indicating better downstream utility\n",
    "\n",
    "If CH index increases but AUC plateaus, the model may be overfitting to train-specific patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\", linewidth=2)\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Val Loss\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Contrastive Loss\")\n",
    "axes[0].set_title(\"Training and Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "# Evaluation happens at epoch 1, then every 5 epochs: 1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50\n",
    "eval_epochs = [1] + list(range(5, num_epochs + 1, 5))\n",
    "print(f\"Eval epochs: {eval_epochs}\")\n",
    "print(f\"Number of evals: {len(eval_epochs)}\")\n",
    "print(f\"History length: {len(history['train_separation'])}\")\n",
    "\n",
    "axes[1].plot(\n",
    "    eval_epochs[: len(history[\"train_separation\"])],\n",
    "    history[\"train_separation\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Train CH Index\",\n",
    "    linewidth=2,\n",
    ")\n",
    "axes[1].plot(\n",
    "    eval_epochs[: len(history[\"val_separation\"])],\n",
    "    history[\"val_separation\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Val CH Index\",\n",
    "    linewidth=2,\n",
    ")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Calinski-Harabasz Index\")\n",
    "axes[1].set_title(\"Calinski-Harabasz Index (Higher is Better)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[2].plot(\n",
    "    eval_epochs[: len(history[\"val_auc\"])],\n",
    "    history[\"val_auc\"],\n",
    "    marker=\"o\",\n",
    "    color=\"green\",\n",
    "    linewidth=2,\n",
    ")\n",
    "axes[2].axhline(y=0.5, color=\"red\", linestyle=\"--\", label=\"Random Baseline\", alpha=0.5)\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Validation AUC\")\n",
    "axes[2].set_title(\"Logistic Regression AUC on Learned Embeddings\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_ylim([0.4, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_history.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Post-Training Embedding Analysis\n",
    "\n",
    "Generate the final debiased embeddings by passing all samples through the trained projection network, then compare to the original embeddings.\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "**In the is_positive plots:**\n",
    "- ✓ Clearer separation between cases (red) and controls (blue)\n",
    "- ✓ Tighter within-class clusters\n",
    "\n",
    "**In the confounder plots:**\n",
    "- ✓ Reduced correlation with confounders (more mixed colors)\n",
    "- ✓ Random scatter rather than clear gradients\n",
    "\n",
    "If confounders still show strong patterns, consider:\n",
    "- Longer training\n",
    "- Stronger weight decay\n",
    "- Explicit adversarial debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell - correct embedding generation happens in cell-36\n",
    "print(\"Skipping - using cell-36 for proper embedding generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate post-training embeddings for all data (current filtered df)\n",
    "model.eval()\n",
    "\n",
    "# Re-extract embeddings from current df state (after filtering in cell-18)\n",
    "print(f\"Extracting embeddings from current df (after filtering)...\")\n",
    "print(f\"Current df shape: {df.shape}\")\n",
    "\n",
    "current_embeddings = extract_embeddings(df, EMBEDDING_COLS)\n",
    "print(\n",
    "    f\"Extracted embeddings shape: {current_embeddings.shape} (n_samples, gene_dim, n_genes)\"\n",
    ")\n",
    "\n",
    "# Convert to tensor and generate trained embeddings\n",
    "all_embeddings = torch.FloatTensor(current_embeddings).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size_inference = 256\n",
    "    trained_embeddings_list = []\n",
    "\n",
    "    for i in range(0, len(all_embeddings), batch_size_inference):\n",
    "        batch = all_embeddings[i : i + batch_size_inference]\n",
    "        projected = model(batch)\n",
    "        trained_embeddings_list.append(projected.cpu().numpy())\n",
    "\n",
    "    trained_embeddings = np.vstack(trained_embeddings_list)\n",
    "\n",
    "print(f\"\\nGenerated trained embeddings: {trained_embeddings.shape}\")\n",
    "print(\n",
    "    f\"Reduced from {current_embeddings.shape[1] * current_embeddings.shape[2]} dimensions to {trained_embeddings.shape[1]} dimensions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on both current original embeddings and trained embeddings\n",
    "# Flatten current_embeddings for PCA\n",
    "current_embeddings_flat = current_embeddings.reshape(current_embeddings.shape[0], -1)\n",
    "\n",
    "# PCA on original (current filtered data)\n",
    "pca_original_current = PCA(n_components=2)\n",
    "pca_coords_original = pca_original_current.fit_transform(current_embeddings_flat)\n",
    "\n",
    "# PCA on trained embeddings\n",
    "pca_trained = PCA(n_components=2)\n",
    "pca_coords_trained = pca_trained.fit_transform(trained_embeddings)\n",
    "\n",
    "# Add to dataframe\n",
    "df[\"PC1_original\"] = pca_coords_original[:, 0]\n",
    "df[\"PC2_original\"] = pca_coords_original[:, 1]\n",
    "df[\"PC1_trained\"] = pca_coords_trained[:, 0]\n",
    "df[\"PC2_trained\"] = pca_coords_trained[:, 1]\n",
    "\n",
    "print(\n",
    "    f\"Original embeddings - Explained variance ratio: {pca_original_current.explained_variance_ratio_}\"\n",
    ")\n",
    "print(\n",
    "    f\"Original embeddings - Total variance explained: {pca_original_current.explained_variance_ratio_.sum():.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nTrained embeddings - Explained variance ratio: {pca_trained.explained_variance_ratio_}\"\n",
    ")\n",
    "print(\n",
    "    f\"Trained embeddings - Total variance explained: {pca_trained.explained_variance_ratio_.sum():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-plot comparison: Original vs Trained for each confounder\n",
    "n_confounders = len(confounder_cols) + 1  # +1 for is_positive\n",
    "n_cols = min(3, n_confounders)\n",
    "n_rows = (n_confounders + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows * 2, n_cols, figsize=(6 * n_cols, 5 * n_rows * 2))\n",
    "axes = axes.flatten() if n_confounders > 1 else [axes]\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "# Plot is_positive first\n",
    "for emb_type, pc1_col, pc2_col, title_suffix in [\n",
    "    (\"original\", \"PC1_original\", \"PC2_original\", \"Original\"),\n",
    "    (\"trained\", \"PC1_trained\", \"PC2_trained\", \"Trained\"),\n",
    "]:\n",
    "    scatter = axes[plot_idx].scatter(\n",
    "        df[pc1_col], df[pc2_col], c=df[LABEL_COL], cmap=\"coolwarm\", alpha=0.6, s=20\n",
    "    )\n",
    "    axes[plot_idx].set_xlabel(\"PC1\")\n",
    "    axes[plot_idx].set_ylabel(\"PC2\")\n",
    "    axes[plot_idx].set_title(f\"PC1 vs PC2 ({title_suffix}) - is_positive\")\n",
    "    axes[plot_idx].legend(*scatter.legend_elements(), title=\"is_positive\", fontsize=8)\n",
    "    plot_idx += 1\n",
    "\n",
    "# Plot each confounder\n",
    "for confounder in confounder_cols:\n",
    "    for emb_type, pc1_col, pc2_col, title_suffix in [\n",
    "        (\"original\", \"PC1_original\", \"PC2_original\", \"Original\"),\n",
    "        (\"trained\", \"PC1_trained\", \"PC2_trained\", \"Trained\"),\n",
    "    ]:\n",
    "        if plot_idx >= len(axes):\n",
    "            break\n",
    "\n",
    "        # Check if confounder is categorical or continuous\n",
    "        if df[confounder].dtype == \"object\" or df[confounder].nunique() < 10:\n",
    "            # Categorical\n",
    "            scatter = axes[plot_idx].scatter(\n",
    "                df[pc1_col],\n",
    "                df[pc2_col],\n",
    "                c=pd.Categorical(df[confounder]).codes,\n",
    "                cmap=\"tab10\",\n",
    "                alpha=0.6,\n",
    "                s=20,\n",
    "            )\n",
    "            axes[plot_idx].legend(\n",
    "                *scatter.legend_elements(), title=confounder, loc=\"best\", fontsize=8\n",
    "            )\n",
    "        else:\n",
    "            # Continuous\n",
    "            scatter = axes[plot_idx].scatter(\n",
    "                df[pc1_col],\n",
    "                df[pc2_col],\n",
    "                c=df[confounder],\n",
    "                cmap=\"viridis\",\n",
    "                alpha=0.6,\n",
    "                s=20,\n",
    "            )\n",
    "            plt.colorbar(scatter, ax=axes[plot_idx], label=confounder)\n",
    "\n",
    "        axes[plot_idx].set_xlabel(\"PC1\")\n",
    "        axes[plot_idx].set_ylabel(\"PC2\")\n",
    "        axes[plot_idx].set_title(f\"PC1 vs PC2 ({title_suffix}) - {confounder}\")\n",
    "        plot_idx += 1\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(plot_idx, len(axes)):\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pca_comparison_all_confounders.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete! Check the saved PNG files for detailed comparisons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Export\n",
    "\n",
    "### Final Metrics\n",
    "Quantify the improvement achieved by contrastive learning:\n",
    "- **Cluster separation improvement**: How much better are cases/controls separated?\n",
    "- **AUC improvement**: Is the embedding more useful for downstream prediction?\n",
    "\n",
    "### Saving Results\n",
    "Optionally save:\n",
    "1. **Trained embeddings**: For use in downstream analyses (GWAS, prediction models, etc.)\n",
    "2. **Model weights**: To apply the same transformation to new samples\n",
    "\n",
    "### Next Steps\n",
    "With the debiased embeddings, you can:\n",
    "- Run association studies with reduced confounding\n",
    "- Train fairer predictive models\n",
    "- Perform clustering or dimensionality reduction with less technical artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is outdated - see cell-41 for correct final summary\n",
    "print(\"See cell-41 for final summary with correct variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on all data\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest validation AUC achieved: {best_val_auc:.4f}\")\n",
    "print(f\"\\nOriginal embeddings:\")\n",
    "print(f\"  Shape: {current_embeddings.shape} (n_samples, gene_dim, n_genes)\")\n",
    "print(f\"  Total features: {current_embeddings.shape[1] * current_embeddings.shape[2]}\")\n",
    "print(\n",
    "    f\"  PCA variance explained (PC1+PC2): {pca_original_current.explained_variance_ratio_.sum():.3f}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTrained embeddings:\")\n",
    "print(f\"  Shape: {trained_embeddings.shape}\")\n",
    "print(\n",
    "    f\"  Dimension reduction: {current_embeddings.shape[1] * current_embeddings.shape[2]} → {trained_embeddings.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"  PCA variance explained (PC1+PC2): {pca_trained.explained_variance_ratio_.sum():.3f}\"\n",
    ")\n",
    "\n",
    "# Compute final metrics on full dataset\n",
    "# Flatten current embeddings for metrics\n",
    "current_embeddings_flat = current_embeddings.reshape(current_embeddings.shape[0], -1)\n",
    "final_ch_original = compute_cluster_separation(\n",
    "    current_embeddings_flat, df[LABEL_COL].values\n",
    ")\n",
    "final_ch_trained = compute_cluster_separation(trained_embeddings, df[LABEL_COL].values)\n",
    "\n",
    "print(f\"\\nCalinski-Harabasz Index (higher is better):\")\n",
    "print(f\"  Original embeddings: {final_ch_original:.2f}\")\n",
    "print(f\"  Trained embeddings: {final_ch_trained:.2f}\")\n",
    "if final_ch_original > 0:\n",
    "    print(\n",
    "        f\"  Improvement: {((final_ch_trained - final_ch_original) / final_ch_original * 100):.2f}%\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"  Improvement: N/A (original CH index was 0)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Files saved:\")\n",
    "print(\"  - pca_original_confounders.png\")\n",
    "print(\"  - training_history.png\")\n",
    "print(\"  - pca_comparison_is_positive.png\")\n",
    "print(\"  - pca_comparison_all_confounders.png\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✨ Genewise architecture works with ANY number of genes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
