{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Intake Agent: VCF File Processing\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the **Source Intake Agent** - the first agent in our multi-source WES pipeline automation system.\n",
    "\n",
    "### Agent Purpose\n",
    "- **Standardize input**: Handle VCF files from different sources (Mayo Tapestry/Helix, Mayo BioBank/Regeneron, UK Biobank)\n",
    "- **Extract metadata**: Identify source, sample counts, reference build\n",
    "- **Validate integrity**: Check file format, corruption, required fields\n",
    "- **Route processing**: Tag files for source-specific downstream processing\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Source Intake Agent                â”‚\n",
    "â”‚  - identify_source()                    â”‚\n",
    "â”‚  - validate_vcf_integrity()             â”‚\n",
    "â”‚  - extract_source_metadata()            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "    bcftools, subprocess\n",
    "```\n",
    "\n",
    "### Test Data\n",
    "- **toy.vcf**: 10,000 lines from 1000 Genomes Chr22 (samples: NA12878, NA12889, NA12890)\n",
    "- Source: 1000 Genomes Phase 3\n",
    "- Build: GRCh37/hg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Setup complete\n",
      "âœ“ Using model: gemini-2.5-flash-lite\n",
      "âœ“ Test VCF: data/toy.vcf\n",
      "âœ“ API key loaded from .env\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from typing import Dict, List, Any, Literal\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Google ADK imports\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.runners import InMemoryRunner\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up paths\n",
    "DATA_DIR = Path(\"data\")\n",
    "TOY_VCF = DATA_DIR / \"toy.vcf\"\n",
    "\n",
    "# API key (if not already set)\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    raise ValueError(\n",
    "        \"GOOGLE_API_KEY not found in environment. Please set it in .env file.\"\n",
    "    )\n",
    "\n",
    "# Model selection\n",
    "LLM_MODEL = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "print(f\"âœ“ Setup complete\")\n",
    "print(f\"âœ“ Using model: {LLM_MODEL}\")\n",
    "print(f\"âœ“ Test VCF: {TOY_VCF}\")\n",
    "print(f\"âœ“ API key loaded from .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool 1: Identify Source\n",
    "\n",
    "Analyzes VCF header and filename to identify data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing identify_source() on toy.vcf...\n",
      "{\n",
      "  \"source\": \"1000genomes_phase3\",\n",
      "  \"n_samples\": 3,\n",
      "  \"sample_ids\": [\n",
      "    \"NA12878\",\n",
      "    \"NA12889\",\n",
      "    \"NA12890\"\n",
      "  ],\n",
      "  \"n_variants\": 9745,\n",
      "  \"build\": \"GRCh37\",\n",
      "  \"vcf_version\": \"VCFv4.1\",\n",
      "  \"source_info\": \"1000GenomesPhase3Pipeline\",\n",
      "  \"vcf_path\": \"data/toy.vcf\",\n",
      "  \"is_gzipped\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def identify_source(vcf_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Identify VCF source from metadata and filename.\n",
    "\n",
    "    Args:\n",
    "        vcf_path: Path to VCF file (can be .vcf or .vcf.gz)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with source, n_samples, n_variants, build, and other metadata\n",
    "    \"\"\"\n",
    "    vcf_path = Path(vcf_path)\n",
    "\n",
    "    if not vcf_path.exists():\n",
    "        return {\"error\": \"File not found\", \"vcf_path\": str(vcf_path)}\n",
    "\n",
    "    # Determine if gzipped\n",
    "    is_gzipped = vcf_path.suffix == \".gz\"\n",
    "\n",
    "    # Extract header\n",
    "    if is_gzipped:\n",
    "        result = subprocess.run(\n",
    "            [\"bcftools\", \"view\", \"-h\", str(vcf_path)], capture_output=True, text=True\n",
    "        )\n",
    "    else:\n",
    "        result = subprocess.run(\n",
    "            [\"grep\", \"^#\", str(vcf_path)], capture_output=True, text=True\n",
    "        )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        return {\"error\": \"Failed to read VCF header\", \"stderr\": result.stderr}\n",
    "\n",
    "    header = result.stdout\n",
    "\n",
    "    # Parse metadata from header\n",
    "    source = \"unknown\"\n",
    "    vcf_name = vcf_path.name.lower()\n",
    "\n",
    "    # Detect source from header and filename\n",
    "    if \"helix\" in header.lower() or \"tapestry\" in vcf_name:\n",
    "        source = \"mayo_tapestry_helix\"\n",
    "    elif \"regeneron\" in header.lower() or \"biobank\" in vcf_name:\n",
    "        source = \"mayo_biobank_regeneron\"\n",
    "    elif \"ukbiobank\" in vcf_name or \"ukb\" in vcf_name:\n",
    "        source = \"ukbiobank\"\n",
    "    elif \"1000genomes\" in header.lower() or \"1kg\" in vcf_name or \"phase3\" in vcf_name:\n",
    "        source = \"1000genomes_phase3\"\n",
    "\n",
    "    # Count samples from header line\n",
    "    header_lines = header.strip().split(\"\\n\")\n",
    "    chrom_line = [l for l in header_lines if l.startswith(\"#CHROM\")]\n",
    "\n",
    "    n_samples = 0\n",
    "    sample_ids = []\n",
    "    if chrom_line:\n",
    "        fields = chrom_line[0].split(\"\\t\")\n",
    "        # Samples start after FORMAT column (index 9)\n",
    "        sample_ids = fields[9:] if len(fields) > 9 else []\n",
    "        n_samples = len(sample_ids)\n",
    "\n",
    "    # Count variants (excluding headers)\n",
    "    if is_gzipped:\n",
    "        count_result = subprocess.run(\n",
    "            f\"bcftools view -H {vcf_path} | wc -l\",\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "    else:\n",
    "        count_result = subprocess.run(\n",
    "            f\"grep -v '^#' {vcf_path} | wc -l\",\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "\n",
    "    n_variants = int(count_result.stdout.strip()) if count_result.returncode == 0 else 0\n",
    "\n",
    "    # Detect reference build\n",
    "    build = \"unknown\"\n",
    "    if \"GRCh38\" in header or \"hg38\" in header:\n",
    "        build = \"GRCh38\"\n",
    "    elif \"GRCh37\" in header or \"hg19\" in header or \"b37\" in header:\n",
    "        build = \"GRCh37\"\n",
    "\n",
    "    # Extract fileformat version\n",
    "    format_line = [l for l in header_lines if l.startswith(\"##fileformat=\")]\n",
    "    vcf_version = format_line[0].split(\"=\")[1] if format_line else \"unknown\"\n",
    "\n",
    "    # Extract source line if present\n",
    "    source_line = [l for l in header_lines if l.startswith(\"##source=\")]\n",
    "    source_info = source_line[0].split(\"=\")[1] if source_line else \"not specified\"\n",
    "\n",
    "    return {\n",
    "        \"source\": source,\n",
    "        \"n_samples\": n_samples,\n",
    "        \"sample_ids\": sample_ids,\n",
    "        \"n_variants\": n_variants,\n",
    "        \"build\": build,\n",
    "        \"vcf_version\": vcf_version,\n",
    "        \"source_info\": source_info,\n",
    "        \"vcf_path\": str(vcf_path),\n",
    "        \"is_gzipped\": is_gzipped,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the function directly\n",
    "print(\"Testing identify_source() on toy.vcf...\")\n",
    "result = identify_source(str(TOY_VCF))\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool 2: Validate VCF Integrity\n",
    "\n",
    "Checks file format compliance, required fields, and basic integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing validate_vcf_integrity() on toy.vcf...\n",
      "{\n",
      "  \"file_exists\": true,\n",
      "  \"has_index\": null,\n",
      "  \"format_valid\": true,\n",
      "  \"has_required_fields\": true,\n",
      "  \"has_fileformat\": true,\n",
      "  \"sample_valid\": true,\n",
      "  \"sample_lines_count\": 100,\n",
      "  \"field_count_match\": true,\n",
      "  \"passed\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def validate_vcf_integrity(vcf_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check VCF file integrity (format compliance, required fields).\n",
    "\n",
    "    Args:\n",
    "        vcf_path: Path to VCF file\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with validation checks and overall pass/fail\n",
    "    \"\"\"\n",
    "    vcf_path = Path(vcf_path)\n",
    "    checks = {}\n",
    "\n",
    "    # 1. Check if file exists\n",
    "    checks[\"file_exists\"] = vcf_path.exists()\n",
    "    if not checks[\"file_exists\"]:\n",
    "        checks[\"passed\"] = False\n",
    "        checks[\"error\"] = \"File not found\"\n",
    "        return checks\n",
    "\n",
    "    # 2. Check if indexed (for .gz files)\n",
    "    is_gzipped = vcf_path.suffix == \".gz\"\n",
    "    if is_gzipped:\n",
    "        checks[\"has_index\"] = (vcf_path.parent / f\"{vcf_path.name}.tbi\").exists() or (\n",
    "            vcf_path.parent / f\"{vcf_path.name}.csi\"\n",
    "        ).exists()\n",
    "    else:\n",
    "        checks[\"has_index\"] = None  # Not applicable for uncompressed\n",
    "\n",
    "    # 3. Check format compliance - try to read header\n",
    "    try:\n",
    "        if is_gzipped:\n",
    "            result = subprocess.run(\n",
    "                [\"bcftools\", \"view\", \"-h\", str(vcf_path)],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "        else:\n",
    "            result = subprocess.run(\n",
    "                [\"grep\", \"^#\", str(vcf_path)],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "        checks[\"format_valid\"] = True\n",
    "        header = result.stdout\n",
    "    except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:\n",
    "        checks[\"format_valid\"] = False\n",
    "        checks[\"error\"] = f\"VCF format invalid or file corrupted: {str(e)}\"\n",
    "        checks[\"passed\"] = False\n",
    "        return checks\n",
    "\n",
    "    # 4. Check required header fields\n",
    "    required_fields = [\"#CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\", \"FILTER\", \"INFO\"]\n",
    "    chrom_line = [l for l in header.split(\"\\n\") if l.startswith(\"#CHROM\")]\n",
    "    if chrom_line:\n",
    "        header_fields = chrom_line[0].split(\"\\t\")\n",
    "        checks[\"has_required_fields\"] = all(f in header_fields for f in required_fields)\n",
    "    else:\n",
    "        checks[\"has_required_fields\"] = False\n",
    "\n",
    "    # 5. Check fileformat declaration\n",
    "    checks[\"has_fileformat\"] = \"##fileformat=VCF\" in header\n",
    "\n",
    "    # 6. Sample a few variants to check data integrity\n",
    "    try:\n",
    "        if is_gzipped:\n",
    "            test = subprocess.run(\n",
    "                [\"bcftools\", \"view\", \"-H\", str(vcf_path), \"|\", \"head\", \"-100\"],\n",
    "                shell=True,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "        else:\n",
    "            test = subprocess.run(\n",
    "                f\"grep -v '^#' {vcf_path} | head -100\",\n",
    "                shell=True,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "\n",
    "        sample_lines = [l for l in test.stdout.strip().split(\"\\n\") if l]\n",
    "        checks[\"sample_valid\"] = len(sample_lines) > 0\n",
    "        checks[\"sample_lines_count\"] = len(sample_lines)\n",
    "\n",
    "        # Check that lines have expected number of fields\n",
    "        if sample_lines and chrom_line:\n",
    "            expected_fields = len(header_fields)\n",
    "            actual_fields = len(sample_lines[0].split(\"\\t\"))\n",
    "            checks[\"field_count_match\"] = actual_fields == expected_fields\n",
    "\n",
    "    except (subprocess.CalledProcessError, subprocess.TimeoutExpired):\n",
    "        checks[\"sample_valid\"] = False\n",
    "        checks[\"field_count_match\"] = False\n",
    "\n",
    "    # Overall pass/fail\n",
    "    required_checks = [\n",
    "        checks[\"format_valid\"],\n",
    "        checks[\"has_required_fields\"],\n",
    "        checks[\"has_fileformat\"],\n",
    "        checks[\"sample_valid\"],\n",
    "    ]\n",
    "\n",
    "    checks[\"passed\"] = all(required_checks)\n",
    "\n",
    "    return checks\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing validate_vcf_integrity() on toy.vcf...\")\n",
    "result = validate_vcf_integrity(str(TOY_VCF))\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool 3: Extract Source-Specific Metadata\n",
    "\n",
    "Extracts detailed metadata for downstream processing based on identified source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing extract_source_metadata() on toy.vcf...\n",
      "{\n",
      "  \"source\": \"1000genomes_phase3\",\n",
      "  \"capture_kit\": \"whole_genome\",\n",
      "  \"expected_depth_range\": [\n",
      "    5,\n",
      "    10\n",
      "  ],\n",
      "  \"expected_ti_tv\": [\n",
      "    2.0,\n",
      "    2.1\n",
      "  ],\n",
      "  \"platform\": \"Illumina (multiple)\",\n",
      "  \"population\": \"Global diversity (26 populations)\",\n",
      "  \"qc_thresholds\": {\n",
      "    \"min_depth\": 4,\n",
      "    \"min_qual\": 20,\n",
      "    \"max_missing_rate\": 0.15\n",
      "  },\n",
      "  \"file_size_mb\": 1.42,\n",
      "  \"chromosomes\": [\n",
      "    \"22\"\n",
      "  ],\n",
      "  \"n_chromosomes\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def extract_source_metadata(vcf_path: str, source: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract source-specific metadata for downstream normalization.\n",
    "\n",
    "    Args:\n",
    "        vcf_path: Path to VCF file\n",
    "        source: Identified source (from identify_source)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with source-specific processing parameters and expectations\n",
    "    \"\"\"\n",
    "    vcf_path = Path(vcf_path)\n",
    "\n",
    "    # Source-specific processing parameters\n",
    "    source_configs = {\n",
    "        \"mayo_tapestry_helix\": {\n",
    "            \"capture_kit\": \"helix_exome_v2\",\n",
    "            \"expected_depth_range\": (20, 30),\n",
    "            \"expected_ti_tv\": (2.8, 3.2),\n",
    "            \"platform\": \"Illumina NovaSeq\",\n",
    "            \"population\": \"Mayo patient population (Midwest US, European ancestry enriched)\",\n",
    "            \"qc_thresholds\": {\n",
    "                \"min_depth\": 10,\n",
    "                \"min_qual\": 30,\n",
    "                \"max_missing_rate\": 0.05,\n",
    "            },\n",
    "        },\n",
    "        \"mayo_biobank_regeneron\": {\n",
    "            \"capture_kit\": \"regeneron_exome\",\n",
    "            \"expected_depth_range\": (30, 40),\n",
    "            \"expected_ti_tv\": (2.8, 3.2),\n",
    "            \"platform\": \"Illumina sequencing (RGC)\",\n",
    "            \"population\": \"Mayo BioBank\",\n",
    "            \"qc_thresholds\": {\n",
    "                \"min_depth\": 15,\n",
    "                \"min_qual\": 30,\n",
    "                \"max_missing_rate\": 0.03,\n",
    "            },\n",
    "        },\n",
    "        \"ukbiobank\": {\n",
    "            \"capture_kit\": \"mixed_illumina_idt\",\n",
    "            \"expected_depth_range\": (15, 25),\n",
    "            \"expected_ti_tv\": (2.7, 3.3),\n",
    "            \"platform\": \"Multiple vendors (Illumina HiSeq, NovaSeq)\",\n",
    "            \"population\": \"UK population (diverse)\",\n",
    "            \"qc_thresholds\": {\"min_depth\": 8, \"min_qual\": 20, \"max_missing_rate\": 0.10},\n",
    "        },\n",
    "        \"1000genomes_phase3\": {\n",
    "            \"capture_kit\": \"whole_genome\",\n",
    "            \"expected_depth_range\": (5, 10),\n",
    "            \"expected_ti_tv\": (2.0, 2.1),  # Whole genome has different Ti/Tv\n",
    "            \"platform\": \"Illumina (multiple)\",\n",
    "            \"population\": \"Global diversity (26 populations)\",\n",
    "            \"qc_thresholds\": {\"min_depth\": 4, \"min_qual\": 20, \"max_missing_rate\": 0.15},\n",
    "        },\n",
    "        \"unknown\": {\n",
    "            \"capture_kit\": \"unknown\",\n",
    "            \"expected_depth_range\": (10, 50),\n",
    "            \"expected_ti_tv\": (2.0, 3.5),\n",
    "            \"platform\": \"unknown\",\n",
    "            \"population\": \"unknown\",\n",
    "            \"qc_thresholds\": {\"min_depth\": 5, \"min_qual\": 20, \"max_missing_rate\": 0.10},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    config = source_configs.get(source, source_configs[\"unknown\"])\n",
    "\n",
    "    # Get basic file info\n",
    "    file_size_mb = vcf_path.stat().st_size / (1024 * 1024)\n",
    "\n",
    "    # Get chromosome coverage\n",
    "    is_gzipped = vcf_path.suffix == \".gz\"\n",
    "    if is_gzipped:\n",
    "        chr_result = subprocess.run(\n",
    "            f\"bcftools view -H {vcf_path} | cut -f1 | sort -u\",\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "    else:\n",
    "        chr_result = subprocess.run(\n",
    "            f\"grep -v '^#' {vcf_path} | cut -f1 | sort -u\",\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "\n",
    "    chromosomes = (\n",
    "        chr_result.stdout.strip().split(\"\\n\") if chr_result.returncode == 0 else []\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"source\": source,\n",
    "        \"capture_kit\": config[\"capture_kit\"],\n",
    "        \"expected_depth_range\": config[\"expected_depth_range\"],\n",
    "        \"expected_ti_tv\": config[\"expected_ti_tv\"],\n",
    "        \"platform\": config[\"platform\"],\n",
    "        \"population\": config[\"population\"],\n",
    "        \"qc_thresholds\": config[\"qc_thresholds\"],\n",
    "        \"file_size_mb\": round(file_size_mb, 2),\n",
    "        \"chromosomes\": chromosomes,\n",
    "        \"n_chromosomes\": len(chromosomes),\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing extract_source_metadata() on toy.vcf...\")\n",
    "result = extract_source_metadata(str(TOY_VCF), \"1000genomes_phase3\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Source Intake Agent\n",
    "\n",
    "Now we'll create the agent that orchestrates these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Source Intake Agent created\n"
     ]
    }
   ],
   "source": [
    "intake_agent = Agent(\n",
    "    name=\"VCF_Source_Intake_Agent\",\n",
    "    model=LLM_MODEL,\n",
    "    instruction=\"\"\"\n",
    "    You are a VCF Source Intake Specialist for genomics pipelines.\n",
    "    \n",
    "    YOUR MISSION:\n",
    "    Process incoming VCF files and prepare them for downstream analysis by:\n",
    "    1. Identifying the data source (Mayo Tapestry/Helix, Mayo BioBank/Regeneron, UK Biobank, 1000 Genomes, etc.)\n",
    "    2. Validating file integrity and format compliance\n",
    "    3. Extracting source-specific metadata for pipeline configuration\n",
    "    4. Flagging any quality concerns or anomalies\n",
    "    \n",
    "    TOOLS AVAILABLE:\n",
    "    - identify_source(vcf_path): Identifies data source, counts samples/variants, detects reference build\n",
    "    - validate_vcf_integrity(vcf_path): Checks file format, required fields, basic integrity\n",
    "    - extract_source_metadata(vcf_path, source): Gets source-specific processing parameters\n",
    "    \n",
    "    WORKFLOW:\n",
    "    1. Always run identify_source() FIRST\n",
    "    2. Then run validate_vcf_integrity()\n",
    "    3. If validation passes, run extract_source_metadata() with the identified source\n",
    "    4. Summarize findings and flag any concerns\n",
    "    \n",
    "    VALIDATION GATES (automatic rejection):\n",
    "    - VCF format invalid\n",
    "    - Missing required fields\n",
    "    - No samples in VCF\n",
    "    - Cannot read file\n",
    "    \n",
    "    HUMAN REVIEW TRIGGERS (flag but don't reject):\n",
    "    - Unknown source detected\n",
    "    - Sample count unusually low (< 10) or high (> 100,000)\n",
    "    - Reference build mismatch with expected\n",
    "    - Missing index file for .gz\n",
    "    \n",
    "    COMMUNICATION STYLE:\n",
    "    - Be systematic and thorough\n",
    "    - Report findings clearly with specific numbers\n",
    "    - Use sections: Source Identification, Validation Results, Metadata Summary, Recommendations\n",
    "    - Flag concerns with âš ï¸ and approvals with âœ“\n",
    "    - If validation fails, STOP and report - do NOT proceed to metadata extraction\n",
    "    \"\"\",\n",
    "    tools=[identify_source, validate_vcf_integrity, extract_source_metadata],\n",
    ")\n",
    "\n",
    "print(\"âœ“ Source Intake Agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent on toy.vcf\n",
    "\n",
    "Let's run the complete intake process on our test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¬ Starting VCF Source Intake Process\n",
      "================================================================================\n",
      "Target file: data/toy.vcf\n",
      "================================================================================\n",
      "\n",
      "\n",
      " ### Created new session: debug_session_id\n",
      "\n",
      "User > \n",
      "Please process this VCF file for intake into our genomics pipeline:\n",
      "\n",
      "File: data/toy.vcf\n",
      "\n",
      "Execute the full intake workflow:\n",
      "1. Identify the data source\n",
      "2. Validate file integrity  \n",
      "3. Extract source-specific metadata\n",
      "4. Provide recommendations for downstream processing\n",
      "\n",
      "Be thorough and flag any concerns.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCF_Source_Intake_Agent > Okay, I will start by identifying the source of the VCF file.\n",
      "\n",
      "Source Identification:\n",
      "*   **Source:** 1000genomes_phase3\n",
      "*   **Sample Count:** 3\n",
      "*   **Variant Count:** 9745\n",
      "*   **Reference Build:** GRCh37\n",
      "*   **VCF Version:** VCFv4.1\n",
      "*   **Sample IDs:** NA12878, NA12889, NA12890\n",
      "*   **Source Info:** 1000GenomesPhase3Pipeline\n",
      "\n",
      "Proceeding to validation.\n",
      "VCF_Source_Intake_Agent > Validation Results:\n",
      "The VCF file `data/toy.vcf` is valid in format and contains the required fields. All checks passed. âœ“\n",
      "\n",
      "Now, I will extract source-specific metadata.\n",
      "VCF_Source_Intake_Agent > Metadata Summary:\n",
      "*   **Source:** 1000genomes_phase3\n",
      "*   **Capture Kit:** whole_genome\n",
      "*   **Platform:** Illumina (multiple)\n",
      "*   **Population:** Global diversity (26 populations)\n",
      "*   **Chromosomes:** 22 (contains only chromosome 22)\n",
      "*   **Expected Depth Range:** 5-10x\n",
      "*   **Expected Ti/Tv Ratio:** 2.0-2.1\n",
      "*   **File Size:** 1.42 MB\n",
      "*   **QC Thresholds:** Max Missing Rate: 0.15, Min Depth: 4, Min Quality: 20\n",
      "\n",
      "Recommendations:\n",
      "The VCF file appears to be a standard subset from the 1000 Genomes phase 3 dataset, containing only chromosome 22. The validation checks passed, and the extracted metadata aligns with expectations for this source.\n",
      "\n",
      "*   **âœ“ Approval:** This VCF file is approved for intake.\n",
      "*   **Downstream Processing:**\n",
      "    *   Ensure your pipeline is configured to handle data from the '1000genomes_phase3' source.\n",
      "    *   Pay attention to the reference build (GRCh37) and chromosome subset (chromosome 22).\n",
      "    *   Utilize the provided QC thresholds and expected ranges for variant quality control during downstream analysis.\n",
      "    *   The sample count is low (3), which is typical for a subset or representative sample VCF. If a larger cohort is expected, this should be noted.\n",
      "\n",
      "================================================================================\n",
      "âœ… INTAKE PROCESS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create runner\n",
    "runner = InMemoryRunner(agent=intake_agent)\n",
    "\n",
    "# Test query\n",
    "query = f\"\"\"\n",
    "Please process this VCF file for intake into our genomics pipeline:\n",
    "\n",
    "File: {TOY_VCF}\n",
    "\n",
    "Execute the full intake workflow:\n",
    "1. Identify the data source\n",
    "2. Validate file integrity  \n",
    "3. Extract source-specific metadata\n",
    "4. Provide recommendations for downstream processing\n",
    "\n",
    "Be thorough and flag any concerns.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ§¬ Starting VCF Source Intake Process\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Target file: {TOY_VCF}\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Run the agent\n",
    "response = await runner.run_debug(query)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… INTAKE PROCESS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Inspection of VCF File\n",
    "\n",
    "Let's also manually inspect the file to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCF Header (first 50 lines):\n",
      "================================================================================\n",
      "##fileformat=VCFv4.1\n",
      "##FILTER=<ID=PASS,Description=\"All filters passed\">\n",
      "##fileDate=20150218\n",
      "##reference=ftp://ftp.1000genomes.ebi.ac.uk//vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz\n",
      "##source=1000GenomesPhase3Pipeline\n",
      "##contig=<ID=1,assembly=b37,length=249250621>\n",
      "##contig=<ID=2,assembly=b37,length=243199373>\n",
      "##contig=<ID=3,assembly=b37,length=198022430>\n",
      "##contig=<ID=4,assembly=b37,length=191154276>\n",
      "##contig=<ID=5,assembly=b37,length=180915260>\n",
      "##contig=<ID=6,assembly=b37,length=171115067>\n",
      "##contig=<ID=7,assembly=b37,length=159138663>\n",
      "##contig=<ID=8,assembly=b37,length=146364022>\n",
      "##contig=<ID=9,assembly=b37,length=141213431>\n",
      "##contig=<ID=10,assembly=b37,length=135534747>\n",
      "##contig=<ID=11,assembly=b37,length=135006516>\n",
      "##contig=<ID=12,assembly=b37,length=133851895>\n",
      "##contig=<ID=13,assembly=b37,length=115169878>\n",
      "##contig=<ID=14,assembly=b37,length=107349540>\n",
      "##contig=<ID=15,assembly=b37,length=102531392>\n",
      "##contig=<ID=16,assembly=b37,length=90354753>\n",
      "##contig=<ID=17,assembly=b37,length=81195210>\n",
      "##contig=<ID=18,assembly=b37,length=78077248>\n",
      "##contig=<ID=19,assembly=b37,length=59128983>\n",
      "##contig=<ID=20,assembly=b37,length=63025520>\n",
      "##contig=<ID=21,assembly=b37,length=48129895>\n",
      "##contig=<ID=22,assembly=b37,length=51304566>\n",
      "##contig=<ID=GL000191.1,assembly=b37,length=106433>\n",
      "##contig=<ID=GL000192.1,assembly=b37,length=547496>\n",
      "##contig=<ID=GL000193.1,assembly=b37,length=189789>\n",
      "##contig=<ID=GL000194.1,assembly=b37,length=191469>\n",
      "##contig=<ID=GL000195.1,assembly=b37,length=182896>\n",
      "##contig=<ID=GL000196.1,assembly=b37,length=38914>\n",
      "##contig=<ID=GL000197.1,assembly=b37,length=37175>\n",
      "##contig=<ID=GL000198.1,assembly=b37,length=90085>\n",
      "##contig=<ID=GL000199.1,assembly=b37,length=169874>\n",
      "##contig=<ID=GL000200.1,assembly=b37,length=187035>\n",
      "##contig=<ID=GL000201.1,assembly=b37,length=36148>\n",
      "##contig=<ID=GL000202.1,assembly=b37,length=40103>\n",
      "##contig=<ID=GL000203.1,assembly=b37,length=37498>\n",
      "##contig=<ID=GL000204.1,assembly=b37,length=81310>\n",
      "##contig=<ID=GL000205.1,assembly=b37,length=174588>\n",
      "##contig=<ID=GL000206.1,assembly=b37,length=41001>\n",
      "##contig=<ID=GL000207.1,assembly=b37,length=4262>\n",
      "##contig=<ID=GL000208.1,assembly=b37,length=92689>\n",
      "##contig=<ID=GL000209.1,assembly=b37,length=159169>\n",
      "##contig=<ID=GL000210.1,assembly=b37,length=27682>\n",
      "##contig=<ID=GL000211.1,assembly=b37,length=166566>\n",
      "##contig=<ID=GL000212.1,assembly=b37,length=186858>\n",
      "##contig=<ID=GL000213.1,assembly=b37,length=164239>\n",
      "\n",
      "(Showing first 50 header lines)\n"
     ]
    }
   ],
   "source": [
    "# Display header lines\n",
    "print(\"VCF Header (first 50 lines):\")\n",
    "print(\"=\" * 80)\n",
    "with open(TOY_VCF, \"r\") as f:\n",
    "    header_count = 0\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            print(line.rstrip())\n",
    "            header_count += 1\n",
    "            if header_count >= 50:\n",
    "                break\n",
    "\n",
    "print(f\"\\n(Showing first {header_count} header lines)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 variant records:\n",
      "================================================================================\n",
      "22\t16050075\t.\tA\tG\t100\tPASS\tAC=0;AF=0.000199681;AN=6;NS=2504;DP=8012;EAS_AF=0;AMR_AF=0;AFR_AF=0;EUR_AF=0;SAS_AF=0.001;AA=.|||;VT=SNP\tGT\t0|0\t0|0\t0|0\n",
      "22\t16050115\t.\tG\tA\t100\tPASS\tAC=0;AF=0.00638978;AN=6;NS=2504;DP=11468;EAS_AF=0;AMR_AF=0.0014;AFR_AF=0.0234;EUR_AF=0;SAS_AF=0;AA=.|||;VT=SNP\tGT\t0|0\t0|0\t0|0\n",
      "22\t16050213\t.\tC\tT\t100\tPASS\tAC=0;AF=0.00758786;AN=6;NS=2504;DP=15092;EAS_AF=0;AMR_AF=0.0014;AFR_AF=0.0272;EUR_AF=0.001;SAS_AF=0;AA=.|||;VT=SNP\tGT\t0|0\t0|0\t0|0\n",
      "22\t16050319\t.\tC\tT\t100\tPASS\tAC=0;AF=0.000199681;AN=6;NS=2504;DP=22609;EAS_AF=0;AMR_AF=0.0014;AFR_AF=0;EUR_AF=0;SAS_AF=0;AA=.|||;VT=SNP\tGT\t0|0\t0|0\t0|0\n",
      "22\t16050527\t.\tC\tA\t100\tPASS\tAC=0;AF=0.000199681;AN=6;NS=2504;DP=23591;EAS_AF=0;AMR_AF=0;AFR_AF=0;EUR_AF=0.001;SAS_AF=0;AA=.|||;VT=SNP\tGT\t0|0\t0|0\t0|0\n"
     ]
    }
   ],
   "source": [
    "# Display first few data lines\n",
    "print(\"\\nFirst 5 variant records:\")\n",
    "print(\"=\" * 80)\n",
    "with open(TOY_VCF, \"r\") as f:\n",
    "    data_count = 0\n",
    "    for line in f:\n",
    "        if not line.startswith(\"#\"):\n",
    "            # Truncate long lines for readability\n",
    "            if len(line) > 200:\n",
    "                print(line[:200].rstrip() + \"...\")\n",
    "            else:\n",
    "                print(line.rstrip())\n",
    "            data_count += 1\n",
    "            if data_count >= 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Let's get some basic statistics about the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCF File Statistics:\n",
      "================================================================================\n",
      "Total header lines: 255\n",
      "Total variant lines: 9745\n",
      "Number of samples: 3\n",
      "Sample IDs: NA12878, NA12889, NA12890\n",
      "File size: 1.42 MB\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "def get_vcf_stats(vcf_path):\n",
    "    stats = {}\n",
    "\n",
    "    with open(vcf_path, \"r\") as f:\n",
    "        header_lines = 0\n",
    "        data_lines = 0\n",
    "        sample_line = None\n",
    "\n",
    "        for line in f:\n",
    "            if line.startswith(\"##\"):\n",
    "                header_lines += 1\n",
    "            elif line.startswith(\"#CHROM\"):\n",
    "                sample_line = line\n",
    "                header_lines += 1\n",
    "            else:\n",
    "                data_lines += 1\n",
    "\n",
    "        stats[\"header_lines\"] = header_lines\n",
    "        stats[\"data_lines\"] = data_lines\n",
    "\n",
    "        if sample_line:\n",
    "            fields = sample_line.strip().split(\"\\t\")\n",
    "            stats[\"samples\"] = fields[9:]\n",
    "            stats[\"n_samples\"] = len(fields[9:])\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "stats = get_vcf_stats(TOY_VCF)\n",
    "\n",
    "print(\"VCF File Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total header lines: {stats['header_lines']}\")\n",
    "print(f\"Total variant lines: {stats['data_lines']}\")\n",
    "print(f\"Number of samples: {stats['n_samples']}\")\n",
    "print(f\"Sample IDs: {', '.join(stats['samples'])}\")\n",
    "print(f\"File size: {TOY_VCF.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we have the Source Intake Agent working, the next agents to implement are:\n",
    "\n",
    "1. **QC/Validation Agent** (Agent 2)\n",
    "   - Sample-level QC (missingness, heterozygosity, Ti/Tv)\n",
    "   - Variant-level QC (missing rate, HWE, MAF)\n",
    "   - Sex concordance checks\n",
    "   - Generate QC reports\n",
    "\n",
    "2. **Harmonization Agent** (Agent 3)\n",
    "   - Normalize variant representation\n",
    "   - Handle different exome capture regions\n",
    "   - Strip to minimal fields\n",
    "\n",
    "3. **Batch Normalization Agent** (Agent 4)\n",
    "   - Detect batch effects via PCA\n",
    "   - Apply ComBat correction if needed\n",
    "   - Preserve population structure\n",
    "\n",
    "4. **H5 Conversion Agent** (Agent 5)\n",
    "   - Convert to PyTorch-ready format\n",
    "   - Validate H5 integrity\n",
    "   - Generate DataLoader example\n",
    "\n",
    "5. **Pipeline Supervisor**\n",
    "   - Orchestrate all agents\n",
    "   - Handle QC gate checkpoints\n",
    "   - Track lineage and provenance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
